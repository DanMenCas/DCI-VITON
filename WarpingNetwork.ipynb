{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxuQhCA2J3vc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Voy5RwYvME"
      },
      "outputs": [],
      "source": [
        "def make_grid(N, iW, iH, device):\n",
        "    grid_x = torch.linspace(-1.0, 1.0, iW).view(1, 1, iW, 1).expand(N, iH, -1, -1).to(device)\n",
        "    grid_y = torch.linspace(-1.0, 1.0, iH).view(1, iH, 1, 1).expand(N, -1, iW, -1).to(device)\n",
        "    grid = torch.cat([grid_x, grid_y], 3)\n",
        "    return grid\n",
        "\n",
        "def save_tensor_as_image(tensor, save_path):\n",
        "    \"\"\"\n",
        "    Saves a [C, H, W] or [1, C, H, W] tensor as an image file.\n",
        "    \"\"\"\n",
        "    if tensor.dim() == 4:\n",
        "        tensor = tensor.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "    to_pil = ToPILImage()\n",
        "    image = to_pil(tensor.cpu().clamp(0, 1))  # Clamp values to [0,1] if needed\n",
        "    image.save(save_path)\n",
        "\n",
        "def flow_loss(flow_list):\n",
        "\n",
        "    loss_tv = 0\n",
        "\n",
        "    for flow in flow_list:\n",
        "      y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
        "      x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
        "      loss_tv = loss_tv + y_tv + x_tv\n",
        "\n",
        "    return loss_tv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX8og9AVYrbO"
      },
      "outputs": [],
      "source": [
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self,layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom1A4x5KmjA"
      },
      "outputs": [],
      "source": [
        "class ResNetEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels, use_dropout=False, use_bn=True, down=True, up=False):\n",
        "      super(ResNetEncoderBlock, self).__init__()\n",
        "\n",
        "      if down:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1)\n",
        "      elif up:\n",
        "          self.scale = nn.Sequential(\n",
        "              nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "              nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "          )\n",
        "      else:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
        "\n",
        "      self.activation = nn.ReLU()\n",
        "\n",
        "      if use_bn:\n",
        "          self.batchnorm = nn.InstanceNorm2d(output_channels)\n",
        "      self.use_bn = use_bn\n",
        "\n",
        "      if use_dropout:\n",
        "          self.dropout = nn.Dropout()\n",
        "      self.use_dropout = use_dropout\n",
        "\n",
        "      self.conv_1 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "      self.conv_2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      residual = self.scale(x)\n",
        "      conv1_x = self.conv_1(residual)\n",
        "      if self.use_bn:\n",
        "          conv1_x = self.batchnorm(conv1_x)\n",
        "      if self.use_dropout:\n",
        "          conv1_x = self.dropout(conv1_x)\n",
        "      conv1_x = self.activation(conv1_x)\n",
        "      conv2_x = self.conv_2(conv1_x)\n",
        "      if self.use_bn:\n",
        "          conv2_x = self.batchnorm(conv2_x)\n",
        "      if self.use_dropout:\n",
        "          conv2_x = self.dropout(conv2_x)\n",
        "\n",
        "      return self.activation(conv2_x + residual)\n",
        "\n",
        "class ClothingEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(ClothingEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "\n",
        "      return x0, x1, x2, x3, x4\n",
        "\n",
        "class SegmentEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(SegmentEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet6 = ResNetEncoderBlock(output_channels * 4, output_channels * 8, down=False)\n",
        "      self.resnet7 = ResNetEncoderBlock(output_channels * 8, output_channels * 4, down=False, up=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "      x5 = self.resnet6(x4)\n",
        "      x6 = self.resnet7(x5)\n",
        "\n",
        "      return x0, x1, x2, x3, x4, x5, x6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WarpingProcess(nn.Module):\n",
        "\n",
        "  def __init__(self, oc):\n",
        "      super(WarpingProcess, self).__init__()\n",
        "\n",
        "      self.conv_after_concat_1 = nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "\n",
        "      self.convs_clothing = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*2, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc, oc*4, kernel_size=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.conv_after_concat_2 = nn.ModuleList([\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment_after_concat = nn.ModuleList([\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*10, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*9, oc*4, down=False, up=True)\n",
        "      ])\n",
        "\n",
        "      self.clothingencoder = ClothingEncoder(4, oc)\n",
        "      self.posencoder = SegmentEncoder(6, oc)\n",
        "\n",
        "  def forward(self, input_1, input_2, device):\n",
        "\n",
        "    clothing_before_warp = []\n",
        "    pose_after_concat = []\n",
        "    clothingpose_before_warp = []\n",
        "\n",
        "    flow_list = []\n",
        "\n",
        "    ce_0, ce_1, ce_2, ce_3, ce_4 = self.clothingencoder(input_1)\n",
        "    clothing_features = [ce_0, ce_1, ce_2, ce_3, ce_4]\n",
        "\n",
        "    pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6 = self.posencoder(input_2)\n",
        "    pose_features = [pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6]\n",
        "\n",
        "    clothing_pose_last_feature = torch.cat([ce_4, pe_4], dim=1)\n",
        "\n",
        "    conv_before_warp = self.conv_after_concat_1(clothing_pose_last_feature)\n",
        "\n",
        "    for i in range(4):\n",
        "\n",
        "      if i == 0:\n",
        "\n",
        "        up = F.interpolate(clothing_features[4], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(conv_before_warp, scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_features[6], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_features[6])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "      else:\n",
        "\n",
        "        up = F.interpolate(clothing_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(clothingpose_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_after_concat[i - 1], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_after_concat[i - 1])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "    last_up = F.interpolate(clothingpose_before_warp[-1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "    grid = make_grid(1, last_up.shape[2], last_up.shape[3], device)\n",
        "\n",
        "    flow_norm = torch.cat([last_up[:, 0:1, :, :] / ((last_up.shape[3] - 1.0) / 2.0), last_up[:, 1:2, :, :] / ((last_up.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "    warped_T1 = F.grid_sample(input_1, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "    flow_list.append(flow_norm)\n",
        "\n",
        "    warped_c = warped_T1[:, :-1, :, :]\n",
        "    warped_cm = warped_T1[:, -1:, :, :]\n",
        "\n",
        "    return warped_c, warped_cm, flow_list\n"
      ],
      "metadata": {
        "id": "BaVhm1GlBbiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCgvCZwc_mLw"
      },
      "outputs": [],
      "source": [
        "size = (256, 256)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(size),        # Resize to fixed size\n",
        "        transforms.ToTensor(),          # Converts to [C, H, W], values in [0, 1]\n",
        "        # transforms.Normalize([0.5]*3, [0.5]*3)  # Optional if using Tanh output\n",
        "    ])\n",
        "\n",
        "def load_image(image_path, size=size, transform=transform):\n",
        "    \"\"\"\n",
        "    Loads a single image and converts it to a tensor of shape [1, 3, H, W]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
        "    image = transform(image)                      # [3, H, W]\n",
        "\n",
        "    tensor = image.unsqueeze(0)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "g6Zk4gihkqaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/images.pth').to(device)\n",
        "cloths = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/cloths.pth').to(device)\n",
        "cloth_masks = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/cloth_masks.pth').to(device)\n",
        "denseposes = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/denseposes.pth').to(device)\n",
        "parse_agnostics = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/parse_agnostics.pth').to(device)\n",
        "warped_masks = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/warped_masks.pth').to(device)\n",
        "agnostics = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/agnostics.pth').to(device)"
      ],
      "metadata": {
        "id": "5AZAIhwJ3DL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = torch.unbind(images, dim=0)\n",
        "cloths = torch.unbind(cloths, dim=0)\n",
        "cloth_masks = torch.unbind(cloth_masks, dim=0)\n",
        "denseposes = torch.unbind(denseposes, dim=0)\n",
        "parse_agnostics = torch.unbind(parse_agnostics, dim=0)\n",
        "warped_masks = torch.unbind(warped_masks, dim=0)\n",
        "agnostics = torch.unbind(agnostics, dim=0)"
      ],
      "metadata": {
        "id": "0UCEVduy3Jh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warped_cloths = [((i + 1 )/ 2)*j for i, j in zip(warped_masks, images)]"
      ],
      "metadata": {
        "id": "9n1_EKzSbNgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [i.unsqueeze(0) for i in images]\n",
        "cloths = [i.unsqueeze(0) for i in cloths]\n",
        "cloth_masks = [i.unsqueeze(0) for i in cloth_masks]\n",
        "denseposes = [i.unsqueeze(0) for i in denseposes]\n",
        "parse_agnostics = [i.unsqueeze(0) for i in parse_agnostics]\n",
        "warped_masks = [i.unsqueeze(0) for i in warped_masks]\n",
        "warped_cloths = [i.unsqueeze(0) for i in warped_cloths]\n",
        "agnostics = [i.unsqueeze(0) for i in agnostics]"
      ],
      "metadata": {
        "id": "SY5YOwEF3K6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = []\n",
        "inputs_2 = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  input_1 = torch.cat([cloths[i], cloth_masks[i][:, 0:1, ...]], dim=1)\n",
        "  input_2 = torch.cat([denseposes[i], parse_agnostics[i]], dim=1)\n",
        "\n",
        "  inputs_1.append(input_1)\n",
        "  inputs_2.append(input_2)"
      ],
      "metadata": {
        "id": "jPouz8KPC3we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNlDo--DaNFd"
      },
      "outputs": [],
      "source": [
        "\n",
        "lr = 0.00005\n",
        "l1_lambda = 1\n",
        "vgg_lambda = 0.1\n",
        "tvlambda = 1\n",
        "epochs = 30\n",
        "criterionL1 = nn.L1Loss().to(device)\n",
        "criterionVGG = VGGLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Va22n5ajiv"
      },
      "outputs": [],
      "source": [
        "generator = WarpingProcess(96).to(device)\n",
        "gen_opt = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3lQ2cltx3cg"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "generator = generator.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH ='/content/drive/MyDrive/AIClothes/DDPM/Models/Warping/3000imgs_warping_lr5e-05_vgg0.1_tvlmabda_1epoch_30.pth'\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(device))\n",
        "\n",
        "generator.load_state_dict(checkpoint['model_state_dict'])\n",
        "gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "losses = checkpoint['loss']"
      ],
      "metadata": {
        "id": "8EuAjqS4K2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN76XwRJlsvt"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "\n",
        "#losses = []\n",
        "\n",
        "for i in range(21, epochs+1):\n",
        "\n",
        "  for input_1, input_2, warped_cloth, warped_mask in zip(inputs_1, inputs_2, warped_cloths, warped_masks):\n",
        "\n",
        "    #input_1, input_2, warped_cloth, warped_mask =  inputs_1[0:11], inputs_2[0:11], warped_cloths[0:11], warped_masks[0:11]\n",
        "\n",
        "    warped_c, warped_cm, flow_list = generator(input_1, input_2, device)\n",
        "\n",
        "    loss_l1_cloth = criterionL1(warped_cm, warped_mask)\n",
        "\n",
        "    loss_vgg = criterionVGG(warped_cm*warped_c, warped_cloth)\n",
        "\n",
        "    loss_tv = flow_loss(flow_list)\n",
        "\n",
        "    for j in range(len(flow_list)-1):\n",
        "      flow = flow_list[j]\n",
        "      N, fH, fW, _ = flow.size()\n",
        "      grid = make_grid(N, input_1.shape[2], input_1.shape[3], device)\n",
        "      flow = F.interpolate(flow.permute(0, 3, 1, 2), size = input_1.shape[2:], mode='bilinear').permute(0, 2, 3, 1)\n",
        "      flow_norm = torch.cat([flow[:, :, :, 0:1] / ((flow.shape[1] - 1.0) / 2.0), flow[:, :, :, 1:2] / ((flow.shape[2] - 1.0) / 2.0)], 3)\n",
        "      warped = F.grid_sample(input_1, grid + flow_norm, padding_mode='border')\n",
        "      warped_c_flow = warped[:, :-1, :, :]\n",
        "      warped_cm_flow = warped[:, -1:, :, :]\n",
        "\n",
        "      loss_l1_cloth += criterionL1(warped_cm_flow, warped_mask) / (2 ** (4-j))\n",
        "      loss_vgg += criterionVGG(warped_cm_flow*warped_c_flow, warped_cloth) / (2 ** (4-j))\n",
        "\n",
        "\n",
        "    loss_G = ((l1_lambda * loss_l1_cloth) + (vgg_lambda * loss_vgg) + (tvlambda * loss_tv))\n",
        "\n",
        "    losses.append(loss_G.item())\n",
        "\n",
        "    gen_opt.zero_grad()\n",
        "    loss_G.backward()\n",
        "    gen_opt.step()\n",
        "\n",
        "    # fig, axs = plt.subplots(1, 7, figsize=(10, 5))\n",
        "    # axs[0].imshow(warped_c[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[0].axis('off')\n",
        "\n",
        "    # axs[1].imshow(warped_cloth[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[1].axis('off')\n",
        "\n",
        "    # axs[2].imshow(warped_cm[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[2].axis('off')\n",
        "\n",
        "    # axs[3].imshow(warped_mask[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[3].axis('off')\n",
        "\n",
        "    # axs[4].imshow(input_1[0, 0:3, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[4].axis('off')\n",
        "\n",
        "    # axs[5].imshow(input_2[0, 0:3, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[5].axis('off')\n",
        "\n",
        "    # axs[6].imshow(input_2[0, 3:6, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    # axs[6].axis('off')\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "  print(f'Epoch: {i}, Mean Loss: {np.mean(losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "        'epoch': i,\n",
        "        'model_state_dict': generator.state_dict(),\n",
        "        'optimizer_state_dict': gen_opt.state_dict(),\n",
        "        'loss': losses\n",
        "    }\n",
        "\n",
        "torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/DDPM/Models/Warping/3000imgs_warping_lr{lr}_vgg{vgg_lambda}_tvlmabda_{tvlambda}epoch_{i}.pth')"
      ],
      "metadata": {
        "id": "5fmYzNFRgYtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(torch.cat(images, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/images.pth')\n",
        "# torch.save(torch.cat(cloths, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/cloths.pth')\n",
        "# torch.save(torch.cat(cloth_masks, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/cloth_masks.pth')\n",
        "# torch.save(torch.cat(denseposes, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/denseposes.pth')\n",
        "# torch.save(torch.cat(parse_agnostics, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/parse_agnostics.pth')\n",
        "# torch.save(torch.cat(warped_masks, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/warped_masks.pth')\n",
        "# torch.save(torch.cat(warped_cloths, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/warped_cloths.pth')\n",
        "# torch.save(torch.cat(agnostics, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/agnostics.pth')"
      ],
      "metadata": {
        "id": "rFfWl9RwZ22u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs_1 = []\n",
        "# inputs_2 = []\n",
        "\n",
        "# for i in range(len(images)):\n",
        "#   input_1 = torch.cat([cloths[i], cloth_masks[i][:, 0:1, ...]], dim=1)\n",
        "#   input_2 = torch.cat([denseposes[i], parse_agnostics[i]], dim=1)\n",
        "\n",
        "#   inputs_1.append(input_1)\n",
        "#   inputs_2.append(input_2)"
      ],
      "metadata": {
        "id": "S92qc4dtr4od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warped_cloth_into_agnostic(agnostic_image, warped_cloth_image, warped_cloth_mask):\n",
        "\n",
        "  agnostic_image_warped_cloth = agnostic_image.clone()\n",
        "\n",
        "  warped_cloth_mask[warped_cloth_mask > 0.5] = 1\n",
        "\n",
        "  warped_cloth_mask = warped_cloth_mask.repeat(1, 3, 1, 1)\n",
        "\n",
        "  agnostic_image_warped_cloth[warped_cloth_mask == 1] = warped_cloth_image[warped_cloth_mask == 1]\n",
        "\n",
        "  return agnostic_image_warped_cloth"
      ],
      "metadata": {
        "id": "mje-hUyReQgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_diffusion_model = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for img in range(len(images)):\n",
        "\n",
        "    agnostic_image = agnostics[img]\n",
        "\n",
        "    warped_c, warped_cm, flow_list = generator(inputs_1[img], inputs_2[img], device)\n",
        "\n",
        "    new_im = warped_cloth_into_agnostic(agnostic_image, warped_c, warped_cm)\n",
        "\n",
        "    new_im = F.interpolate(new_im, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "    inputs_diffusion_model.append(new_im)\n",
        "\n",
        "  #save_tensor_as_image(new_im[0], \"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/inputs_difussion_model/\" + list_names[img])"
      ],
      "metadata": {
        "id": "y7x1RugJgJki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(inputs_diffusion_model)):\n",
        "  plt.imshow((inputs_diffusion_model[i][0].permute(1, 2, 0).detach().cpu().numpy() + 1)/2)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jBpXC1f7_B-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_sample_inputs_diffusion_model = random.sample([i for i in range(3000)], 1500)"
      ],
      "metadata": {
        "id": "lTDVtmM8stYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_inputs_diffusion_model = []\n",
        "\n",
        "for i in list_sample_inputs_diffusion_model:\n",
        "  sample_inputs_diffusion_model.append(inputs_diffusion_model[i])"
      ],
      "metadata": {
        "id": "jlYYvk6juc8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(torch.cat(sample_inputs_diffusion_model, dim=0), '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/input_difussion_model_sample.pth')"
      ],
      "metadata": {
        "id": "2vV1crApZKi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(list_sample_inputs_diffusion_model, '/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/list_sample_1500_images.pth')"
      ],
      "metadata": {
        "id": "zmFthkCWv-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_names = os.listdir(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/cloth\")\n",
        "\n",
        "test_inputs_1 = []\n",
        "test_inputs_2 = []\n",
        "\n",
        "\n",
        "for i in list_names:\n",
        "\n",
        "  test_cloth = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/cloth/\" + i).to(device)\n",
        "\n",
        "  test_cloth_mask = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/cloth_mask/\" + i).to(device)\n",
        "\n",
        "  test_densepose = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/densepose/\" + i).to(device)\n",
        "\n",
        "  test_parse_agnostic = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/parse_agnostic/\" + i.split('.')[0] + \".png\").to(device)\n",
        "\n",
        "  test_agnostic = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/agnostic/\" + i).to(device)\n",
        "\n",
        "  test_input_1 = torch.cat([test_cloth, test_cloth_mask[:, 0:1, ...]], dim=1)\n",
        "  test_input_2 = torch.cat([test_densepose, test_parse_agnostic], dim=1)\n",
        "\n",
        "  test_inputs_1.append(test_input_1)\n",
        "  test_inputs_2.append(test_input_2)"
      ],
      "metadata": {
        "id": "-ZpfComjlLTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_warped_c, test_warped_cm, test_flow_list = generator(test_input_1, test_input_2, device)"
      ],
      "metadata": {
        "id": "jdjxb5GPlkN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_warped_c[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efmrMcwhl41f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_im = warped_cloth_into_agnostic(test_agnostic, test_warped_c, test_warped_cm)\n",
        "\n",
        "plt.imshow(new_im[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRXs3c6MADet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_reshaped = F.interpolate(new_im, size=(512, 512), mode='bilinear', align_corners=False)"
      ],
      "metadata": {
        "id": "VREJRKPICLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im_reshaped[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ni-ruhaOASzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_tensor_as_image(im_reshaped[0], \"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/inputs_difussion_model/\" + i)"
      ],
      "metadata": {
        "id": "KHmURkjGoOjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i"
      ],
      "metadata": {
        "id": "p1VqWkgqOem_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}