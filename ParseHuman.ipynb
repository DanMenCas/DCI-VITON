{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "\n",
        "!git clone 'https://github.com/facebookresearch/detectron2' detectron2_repo\n",
        "dist = distutils.core.run_setup(\"./detectron2_repo/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "\n",
        "!pip install git+https://github.com/facebookresearch/detectron2@main#subdirectory=projects/DensePose\n",
        "\n",
        "!pip install rembg\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "fbEnYv0Cbkm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import torch, cv2\n",
        "import os\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from google.colab.patches import cv2_imshow\n",
        "from rembg import remove, new_session\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "from os import path as osp\n",
        "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from densepose.config import add_densepose_config\n",
        "from densepose.vis.densepose_results import DensePoseResultsFineSegmentationVisualizer\n",
        "from densepose.vis.bounding_box import ScoredBoundingBoxVisualizer\n",
        "from densepose.vis.base import CompoundVisualizer\n",
        "from densepose.vis.extractor import create_extractor, CompoundExtractor"
      ],
      "metadata": {
        "id": "KLC_w6M0kGNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing():\n",
        "\n",
        "  def __init__(self, person_image_path, cloth_image_path):\n",
        "\n",
        "    self.person_image_path = person_image_path\n",
        "    self.cloth_image_path = cloth_image_path\n",
        "    self.img = Image.open(person_image_path)\n",
        "    self.cloth_img = Image.open(cloth_image_path)\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "  def key_points(self):\n",
        "\n",
        "    self.image = np.array(self.img)\n",
        "\n",
        "    # Load pretrained COCO keypoint model\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.MODEL.DEVICE = self.device\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    outputs = predictor(self.image)\n",
        "    keypoints = outputs[\"instances\"].pred_keypoints.cpu().numpy()\n",
        "\n",
        "    #0.Nose\n",
        "    #1.Right eye\n",
        "    #2.Left eye\n",
        "    #3.Right ear\n",
        "    #4.Left ear\n",
        "    #5.Right shoulder\n",
        "    #6.Left Shoulder\n",
        "    #7.Right elbow\n",
        "    #8.Left elbow\n",
        "    #9.Right wrist\n",
        "    #10.Left wrist\n",
        "    #11.Right hip\n",
        "    #12.Left hip\n",
        "    #13.Right knee\n",
        "    #14.Left Knee\n",
        "    #15.Right Wrist\n",
        "    #16.Left Wrist\n",
        "\n",
        "    kpoints = keypoints[0][:, 0:2]\n",
        "\n",
        "    keypoints_chest = (kpoints[5]+kpoints[6])/2\n",
        "\n",
        "    keypoints_chest_reshaped = keypoints_chest.reshape(1, 2)\n",
        "\n",
        "    kpoints_chest = np.vstack([kpoints, keypoints_chest_reshaped])\n",
        "\n",
        "    # Visualize\n",
        "    # v = Visualizer(image[:, :, ::-1], scale=1.0)\n",
        "    # out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    # cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "    return kpoints_chest\n",
        "\n",
        "  def parse_human(self):\n",
        "\n",
        "    # ---- Config ----\n",
        "    model_name = \"yolo12138/segformer-b2-human-parse-24\"\n",
        "\n",
        "    # ---- Load model ----\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    processor = SegformerImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModelForSemanticSegmentation.from_pretrained(model_name).to(device).eval()\n",
        "\n",
        "    # ---- Load and preprocess image ----\n",
        "    inputs = processor(images=self.img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # ---- Inference ----\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # ---- Resize to original size ----\n",
        "    orig_size = self.img.size  # (width, height)\n",
        "    logits_upsampled = F.interpolate(\n",
        "        logits, size=(orig_size[1], orig_size[0]),\n",
        "        mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    seg_map = logits_upsampled.argmax(dim=1)[0].cpu().numpy()\n",
        "\n",
        "    # ---- LIP 20-class color palette ---- (85,255,170), (170,255,85)\n",
        "    # (face=blue, hair=red, upper-clothes=orange, arms=cyan, pants=teal, etc.)\n",
        "    LIP_COLORS = np.array([\n",
        "        [0, 0, 0],       # 0 background\n",
        "        [1, 1, 1],       # 1 hat\n",
        "        [2, 2, 2],       # 2 hair\n",
        "        [3, 3, 3],       # 3 glove\n",
        "        [4, 4, 4],       # 4 glasses\n",
        "        [5, 5, 5],       # 5 upper clothes\n",
        "        [6, 6, 6],       # 6 dress\n",
        "        [7, 7, 7],       # 7 coat\n",
        "        [8, 8, 8],       # 8 socks\n",
        "        [9, 9, 9],       # 9 left_pants\n",
        "        [9, 9, 9],       # 10 right_pants\n",
        "        [10, 10, 10],    # 11 skin_around_neck_region\n",
        "        [11, 11, 11],    # 12 scarf\n",
        "        [12, 12, 12],    # 13 skirt\n",
        "        [13, 13, 13],    # 14 Face\n",
        "        [14, 14, 14],    # 15 left arm\n",
        "        [15, 15, 15],    # 16 right arm\n",
        "        [16, 16, 16],    # 17 left leg\n",
        "        [17, 17, 17],    # 18 right leg\n",
        "        [18, 18, 18],    # 19 left shoe\n",
        "        [19, 19, 19],    # 20 right shoe\n",
        "        [5, 5, 5],       # 21 left_sleeve_for_upper\n",
        "        [5, 5, 5],       # 22 right_sleeve_for_upper\n",
        "        [0, 0, 0]        # 23 bag\n",
        "    ], dtype=np.uint8)\n",
        "\n",
        "    # ---- Apply palette ----\n",
        "    seg_colored = LIP_COLORS[seg_map % len(LIP_COLORS)]\n",
        "\n",
        "    # # ---- Display ----\n",
        "    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    # axes[0].imshow(img_np)\n",
        "    # axes[0].set_title(\"Original\")\n",
        "    # axes[0].axis(\"off\")\n",
        "\n",
        "    # axes[1].imshow(seg_colored)\n",
        "    # axes[1].set_title(\"LIP-style Segmentation Map\")\n",
        "    # axes[1].axis(\"off\")\n",
        "\n",
        "    # axes[2].imshow(overlay)\n",
        "    # axes[2].set_title(\"Overlay\")\n",
        "    # axes[2].axis(\"off\")\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    return seg_colored\n",
        "\n",
        "  def get_agnostic_and_mask(self):\n",
        "\n",
        "    im = self.img\n",
        "    im_parse = self.parse_human()[:, :, 0]\n",
        "    pose_data = self.key_points()\n",
        "\n",
        "    parse_array = np.array(im_parse)\n",
        "    parse_head = ((parse_array == 4).astype(np.float32) +\n",
        "                  (parse_array == 13).astype(np.float32))\n",
        "    parse_lower = ((parse_array == 9).astype(np.float32) +\n",
        "                  (parse_array == 12).astype(np.float32) +\n",
        "                  (parse_array == 16).astype(np.float32) +\n",
        "                  (parse_array == 17).astype(np.float32) +\n",
        "                  (parse_array == 18).astype(np.float32) +\n",
        "                  (parse_array == 19).astype(np.float32))\n",
        "\n",
        "    # Initialize agnostic image and mask\n",
        "    agnostic = im.copy()\n",
        "    mask = Image.new('L', im.size, 0)  # Binary mask: black background\n",
        "    agnostic_draw = ImageDraw.Draw(agnostic)\n",
        "    mask_draw = ImageDraw.Draw(mask)\n",
        "\n",
        "    # Normalize arm lengths\n",
        "    length_a = np.linalg.norm(pose_data[5] - pose_data[6])\n",
        "    length_b = np.linalg.norm(pose_data[11] - pose_data[12])\n",
        "    point = (pose_data[12] + pose_data[11]) / 2\n",
        "    pose_data[12] = point + (pose_data[12] - point) / length_b * length_a\n",
        "    pose_data[11] = point + (pose_data[11] - point) / length_b * length_a\n",
        "\n",
        "    r = int(length_a / 16) + 1\n",
        "\n",
        "    # Mask torso\n",
        "    for i in [12, 11]:\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*3, pointy-r*6, pointx+r*3, pointy+r*6), 'gray', 'gray')\n",
        "        mask_draw.ellipse((pointx-r*3, pointy-r*6, pointx+r*3, pointy+r*6), 255, 255)\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [6, 12]], 'gray', width=r*6)\n",
        "    mask_draw.line([tuple(pose_data[i]) for i in [6, 12]], 255, width=r*6)\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [5, 11]], 'gray', width=r*6)\n",
        "    mask_draw.line([tuple(pose_data[i]) for i in [5, 11]], 255, width=r*6)\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [12, 11]], 'gray', width=r*12)\n",
        "    mask_draw.line([tuple(pose_data[i]) for i in [12, 11]], 255, width=r*12)\n",
        "    agnostic_draw.polygon([tuple(pose_data[i]) for i in [6, 5, 11, 12]], 'gray', 'gray')\n",
        "    mask_draw.polygon([tuple(pose_data[i]) for i in [6, 5, 11, 12]], 255, 255)\n",
        "\n",
        "    # Mask neck\n",
        "    pointx, pointy = pose_data[17]\n",
        "    agnostic_draw.rectangle((pointx-r*5, pointy-r*9, pointx+r*5, pointy), 'gray', 'gray')\n",
        "    mask_draw.rectangle((pointx-r*5, pointy-r*9, pointx+r*5, pointy), 255, 255)\n",
        "\n",
        "    # Mask arms\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [6, 5]], 'gray', width=r*12)\n",
        "    mask_draw.line([tuple(pose_data[i]) for i in [6, 5]], 255, width=r*12)\n",
        "    for i in [6, 5]:\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 'gray', 'gray')\n",
        "        mask_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 255, 255)\n",
        "    for i in [8, 10, 7, 9]:\n",
        "        if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "            continue\n",
        "        agnostic_draw.line([tuple(pose_data[j]) for j in [i - 2, i]], 'gray', width=r*10)\n",
        "        mask_draw.line([tuple(pose_data[j]) for j in [i - 2, i]], 255, width=r*10)\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'gray', 'gray')\n",
        "        mask_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 255, 255)\n",
        "\n",
        "    # Handle arm masks for parsing\n",
        "    for parse_id, pose_ids in [(14, [5, 7, 9]), (15, [6, 8, 10])]:\n",
        "        mask_arm = Image.new('L', (768, 1024), 255)  # White background\n",
        "        mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "        pointx, pointy = pose_data[pose_ids[0]]\n",
        "        mask_arm_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 0, 0)\n",
        "        for i in pose_ids[1:]:\n",
        "            if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            mask_arm_draw.line([tuple(pose_data[j]) for j in [i - 2, i]], 0, width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            if i != pose_ids[-1]:\n",
        "                mask_arm_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 0, 0)\n",
        "        mask_arm_draw.ellipse((pointx-r*4, pointy-r*4, pointx+r*4, pointy+r*4), 0, 0)\n",
        "\n",
        "        parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "        agnostic.paste(im, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "    # Paste head and lower body\n",
        "    agnostic.paste(im, None, Image.fromarray(np.uint8(parse_head * 255), 'L'))\n",
        "    agnostic.paste(im, None, Image.fromarray(np.uint8(parse_lower * 255), 'L'))\n",
        "\n",
        "    # Convert mask and agnostic to NumPy arrays\n",
        "    parse_array[~np.isin(parse_array, [4, 13, 9, 12, 16, 17, 18, 19])] = 1\n",
        "    parse_array[np.isin(parse_array, [4, 13, 9, 12, 16, 17, 18, 19])] = 0\n",
        "\n",
        "    mask = np.array(mask)  # Binary mask (0 for background, 255 for masked areas)\n",
        "    agnostic = np.array(agnostic)\n",
        "\n",
        "    mask = mask * parse_array\n",
        "\n",
        "    return agnostic, mask\n",
        "\n",
        "  def get_parse_agnostic(self):\n",
        "\n",
        "    im_parse = Image.fromarray(self.parse_human()[:, :, 0])\n",
        "    pose_data = self.key_points()\n",
        "\n",
        "    parse_array = np.array(im_parse)\n",
        "    parse_head = ((parse_array == 2).astype(np.float32) +\n",
        "                  (parse_array == 4).astype(np.float32)+\n",
        "                  (parse_array == 13).astype(np.float32))\n",
        "    parse_lower = ((parse_array == 9).astype(np.float32) +\n",
        "                  (parse_array == 12).astype(np.float32) +\n",
        "                  (parse_array == 16).astype(np.float32) +\n",
        "                  (parse_array == 17).astype(np.float32) +\n",
        "                  (parse_array == 18).astype(np.float32) +\n",
        "                  (parse_array == 19).astype(np.float32))\n",
        "\n",
        "    agnostic = im_parse.copy()\n",
        "    agnostic_draw = ImageDraw.Draw(agnostic)\n",
        "\n",
        "    length_a = np.linalg.norm(pose_data[5] - pose_data[6])\n",
        "    length_b = np.linalg.norm(pose_data[11] - pose_data[12])\n",
        "    point = (pose_data[12] + pose_data[11]) / 2\n",
        "    pose_data[12] = point + (pose_data[12] - point) / length_b * length_a\n",
        "    pose_data[11] = point + (pose_data[11] - point) / length_b * length_a\n",
        "\n",
        "    r = int(length_a / 16) + 1\n",
        "\n",
        "    # mask torso\n",
        "    for i in [12, 11]:\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*3, pointy-r*6, pointx+r*3, pointy+r*6), 'gray', 'gray')\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [6, 12]], 'gray', width=r*6)\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [5, 11]], 'gray', width=r*6)\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [12, 11]], 'gray', width=r*12)\n",
        "    agnostic_draw.polygon([tuple(pose_data[i]) for i in [6, 5, 11, 12]], 'gray', 'gray')\n",
        "\n",
        "    #mask neck\n",
        "    pointx, pointy = pose_data[17]\n",
        "    agnostic_draw.rectangle((pointx-r*5, pointy-r*9, pointx+r*5, pointy), 'gray', 'gray')\n",
        "\n",
        "    # mask arms\n",
        "    agnostic_draw.line([tuple(pose_data[i]) for i in [6, 5]], 'gray', width=r*12)\n",
        "    for i in [6, 5]:\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 'gray', 'gray')\n",
        "    for i in [8, 10, 7, 9]:\n",
        "        if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "            continue\n",
        "        agnostic_draw.line([tuple(pose_data[j]) for j in [i - 2, i]], 'gray', width=r*10)\n",
        "        pointx, pointy = pose_data[i]\n",
        "        agnostic_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'gray', 'gray')\n",
        "\n",
        "    for parse_id, pose_ids in [(14, [5, 7, 9]), (15, [6, 8, 10])]:\n",
        "        mask_arm = Image.new('L', (768, 1024), 'white')\n",
        "        mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "        pointx, pointy = pose_data[pose_ids[0]]\n",
        "        mask_arm_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 'black', 'black')\n",
        "        for i in pose_ids[1:]:\n",
        "            if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            mask_arm_draw.line([tuple(pose_data[j]) for j in [i - 2, i]], 'black', width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            if i != pose_ids[-1]:\n",
        "                mask_arm_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'black', 'black')\n",
        "        mask_arm_draw.ellipse((pointx-r*4, pointy-r*4, pointx+r*4, pointy+r*4), 'black', 'black')\n",
        "\n",
        "        parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "        agnostic.paste(im_parse, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "    agnostic.paste(im_parse, None, Image.fromarray(np.uint8(parse_head * 255), 'L'))\n",
        "    agnostic.paste(im_parse, None, Image.fromarray(np.uint8(parse_lower * 255), 'L'))\n",
        "\n",
        "    parse_agnostic = np.array(agnostic)\n",
        "\n",
        "    #This is for create the same img with three channels\n",
        "\n",
        "    parse_agnostic_3_ch = np.repeat(parse_agnostic[:, :, np.newaxis], repeats=3, axis=2)\n",
        "\n",
        "    return parse_agnostic_3_ch\n",
        "\n",
        "  def open_pose(self):\n",
        "\n",
        "    img_path = self.person_image_path\n",
        "\n",
        "    img = read_image(img_path, format=\"BGR\")\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    add_densepose_config(cfg)\n",
        "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "    cfg.merge_from_file(\"/content/detectron2_repo/projects/DensePose/configs/densepose_rcnn_R_50_FPN_s1x.yaml\")\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set threshold for this model\n",
        "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "    cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\"\n",
        "    #cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_s1x/251155172/model_final_c4ea5f.pkl\"\n",
        "    # Set the device to CPU if a CUDA enabled GPU is not available\n",
        "    cfg.MODEL.DEVICE = self.device\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "    outputs = predictor(img)['instances']\n",
        "\n",
        "    dp_segm_vis = DensePoseResultsFineSegmentationVisualizer(cfg=cfg)\n",
        "    visualizer = CompoundVisualizer([dp_segm_vis])\n",
        "    extractor = CompoundExtractor([\n",
        "        create_extractor(dp_segm_vis),\n",
        "    ])\n",
        "\n",
        "    image_zero = np.zeros_like(img)\n",
        "    data = extractor(outputs)\n",
        "    image_vis = visualizer.visualize(image_zero, data)\n",
        "\n",
        "    # Normalize the image to the range of 0 to 255\n",
        "    # First, ensure the image is a floating-point type for calculations\n",
        "    image = image_vis.astype(np.float32)\n",
        "\n",
        "    # Find the minimum and maximum pixel values\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    normalized_image = (image - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Scale the normalized image to the range [0, 255] and convert to uint8\n",
        "    image_normalized_255 = (normalized_image * 255).astype(np.uint8)\n",
        "\n",
        "    # You can now use image_normalized_255 for display or further processing\n",
        "    # For example, to display the normalized image:\n",
        "    # cv2_imshow(image_normalized_255)\n",
        "\n",
        "    return image_normalized_255\n",
        "\n",
        "  def cloth_mask(self):\n",
        "\n",
        "    input_img = self.cloth_img\n",
        "\n",
        "    # Specify the model to use with GPU\n",
        "    model_name = \"u2net\"  # u2net supports GPU acceleration\n",
        "    session = new_session(model_name, providers=[\"CUDAExecutionProvider\"])  # Enable GPU\n",
        "\n",
        "    # Generate the mask (white for the object/cloth, black for background) using GPU\n",
        "    mask = remove(input_img, session=session, only_mask=True)\n",
        "\n",
        "    return np.array(mask)"
      ],
      "metadata": {
        "id": "IVE1QkHSY-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  img_path = \"00055_00.jpg\"\n",
        "  cloth_path = \"00067_00.jpg\"\n",
        "\n",
        "  img_preprocess = Preprocessing(img_path, cloth_path)\n",
        "\n",
        "  open_pose = img_preprocess.open_pose()\n",
        "\n",
        "  key_points = img_preprocess.key_points()\n",
        "\n",
        "  parse_human = img_preprocess.parse_human()\n",
        "\n",
        "  agnostic, agnostic_mask = img_preprocess.get_agnostic_and_mask()\n",
        "\n",
        "  parse_agnostic = img_preprocess.get_parse_agnostic()\n",
        "  parse_agnostic[parse_agnostic != parse_human] = 0\n",
        "\n",
        "  cloth_mask = img_preprocess.cloth_mask()\n",
        "\n",
        "  %matplotlib inline\n",
        "\n",
        "  plt.imshow(agnostic)\n",
        "  plt.show()\n",
        "  plt.imshow(agnostic_mask)\n",
        "  plt.show()\n",
        "  plt.imshow(parse_agnostic)\n",
        "  plt.show()\n",
        "  plt.imshow(open_pose[:, :, ::-1])\n",
        "  plt.show()\n",
        "  plt.imshow(cloth_mask)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "aFKZjOURPfSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08wDAvULrVZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}