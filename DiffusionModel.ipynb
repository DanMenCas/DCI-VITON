{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLfV4UqLlRgM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from diffusers import AutoencoderKL\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zeCr7RISi8i"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN1NqjeGV55e"
      },
      "outputs": [],
      "source": [
        "size = (512, 512)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "def load_image(image_path, transform=transform):\n",
        "    \"\"\"\n",
        "    Loads a single image and converts it to a tensor of shape [1, 3, H, W]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "\n",
        "    tensor = image.unsqueeze(0)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7W8dgmON08i"
      },
      "outputs": [],
      "source": [
        "class NoiseScheduler:\n",
        "    def __init__(self, timesteps=1000, beta_schedule=\"linear\", device='cpu'):\n",
        "        self.timesteps = timesteps\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        if beta_schedule == \"linear\":\n",
        "            betas = self._linear_beta_schedule(timesteps)\n",
        "        elif beta_schedule == \"cosine\":\n",
        "            betas = self._cosine_beta_schedule(timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown beta schedule: {beta_schedule}\")\n",
        "\n",
        "        self.betas = betas.to(self.device)\n",
        "        self.alphas = (1.0 - self.betas).to(self.device)\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(self.device)\n",
        "\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(self.device)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod).to(self.device)\n",
        "\n",
        "    def _linear_beta_schedule(self, timesteps, start=0.0001, end=0.02):\n",
        "        return torch.linspace(start, end, timesteps, dtype=torch.float32)\n",
        "\n",
        "    def _cosine_beta_schedule(self, timesteps, s=0.008):\n",
        "        t = torch.arange(timesteps + 1, dtype=torch.float32)\n",
        "        f_t = torch.cos(((t / timesteps) + s) / (1 + s) * math.pi / 2) ** 2\n",
        "        alphas_bar = f_t / f_t[0]\n",
        "        betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
        "        return torch.clip(betas, 0.0001, 0.999)\n",
        "\n",
        "    def get_noisy_image(self, x_0, t, noise=None):\n",
        "        \"\"\"\n",
        "        Adds noise to the original image x_0 at time t.\n",
        "        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0, device=self.device)\n",
        "        if t.ndim == 1:\n",
        "            alpha_bar_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        else:\n",
        "            alpha_bar_t = self.alphas_cumprod[t]\n",
        "\n",
        "        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
        "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)\n",
        "\n",
        "        x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    def denoise_image(self, x_t, t, noise_unet):\n",
        "        \"\"\"\n",
        "        Denoises an image x_t given the predicted noise (noise_unet) at time t.\n",
        "        This is the reverse step.\n",
        "        \"\"\"\n",
        "        if t.ndim == 1:\n",
        "            alpha_bar_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        else:\n",
        "            alpha_bar_t = self.alphas_cumprod[t]\n",
        "\n",
        "        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
        "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)\n",
        "        x_0 = (x_t - sqrt_one_minus_alpha_bar_t * noise_unet) / sqrt_alpha_bar_t\n",
        "\n",
        "        return x_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwdfRbIsOa_J"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal Positional Embedding for time steps.\n",
        "    Transforms a scalar time step 't' into a high-dimensional vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.half_dim = dim // 2\n",
        "        self.embeddings = math.log(10000) / (self.half_dim - 1)\n",
        "        self.embeddings = torch.exp(torch.arange(self.half_dim) * -self.embeddings)\n",
        "\n",
        "    def forward(self, time):\n",
        "        time_embedding = time.unsqueeze(1) * self.embeddings.to(time.device)\n",
        "\n",
        "        time_embedding = torch.cat((time_embedding.sin(), time_embedding.cos()), dim=-1)\n",
        "        return time_embedding\n",
        "\n",
        "class TimeEmbeddingMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP to process the sinusoidal time embedding into scale and shift parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wii3V9viNsv-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdaptiveGroupNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Group Normalization (AdaGN) layer.\n",
        "    Applies GroupNorm, then scales and shifts features based on conditioning (time embedding).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_groups, num_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(num_groups, num_channels, affine=False)\n",
        "        self.time_proj = nn.Linear(time_emb_dim, 2 * num_channels)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "\n",
        "        normed_x = self.norm(x)\n",
        "\n",
        "        scale_shift = self.time_proj(time_emb)\n",
        "        scale, shift = scale_shift.chunk(2, dim=1)\n",
        "\n",
        "        scale = scale.unsqueeze(-1).unsqueeze(-1)\n",
        "        shift = shift.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        output = normed_x * (1 + scale) + shift\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN6b1VTLNyPn"
      },
      "outputs": [],
      "source": [
        "class ContractingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, use_dropout=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.norm1 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        identity = self.residual_conv(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, time_emb)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, time_emb)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.downsample(x + identity)\n",
        "        return x\n",
        "\n",
        "class ExpandingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, use_dropout=False):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.activation = nn.SiLU()\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, skip_con_x, time_emb):\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        if x.shape != skip_con_x.shape:\n",
        "             x = torch.nn.functional.interpolate(x, size=skip_con_x.shape[2:], mode='nearest')\n",
        "\n",
        "        x = torch.cat([x, skip_con_x], dim=1)\n",
        "\n",
        "        identity = self.residual_conv(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, time_emb)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, time_emb)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.activation(x)\n",
        "        return x + identity\n",
        "\n",
        "class FeatureMapBlock(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjYudbrBN3oa"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, hidden_channels=64, time_emb_dim=256): # Added time_emb_dim\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionalEmbedding(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
        "        )\n",
        "        self.time_emb_dim = time_emb_dim\n",
        "\n",
        "\n",
        "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
        "\n",
        "\n",
        "        self.contract1 = ContractingBlock(hidden_channels, hidden_channels * 2, time_emb_dim)\n",
        "        self.contract2 = ContractingBlock(hidden_channels * 2, hidden_channels * 4, time_emb_dim)\n",
        "        self.contract3 = ContractingBlock(hidden_channels * 4, hidden_channels * 8, time_emb_dim)\n",
        "        self.contract4 = ContractingBlock(hidden_channels * 8, hidden_channels * 16, time_emb_dim)\n",
        "        self.contract5 = ContractingBlock(hidden_channels * 16, hidden_channels * 32, time_emb_dim)\n",
        "        self.contract6 = ContractingBlock(hidden_channels * 32, hidden_channels * 64, time_emb_dim)\n",
        "\n",
        "\n",
        "        self.expand0 = ExpandingBlock(hidden_channels * 64, hidden_channels * 32, time_emb_dim)\n",
        "        self.expand1 = ExpandingBlock(hidden_channels * 32, hidden_channels * 16, time_emb_dim)\n",
        "        self.expand2 = ExpandingBlock(hidden_channels * 16, hidden_channels * 8, time_emb_dim)\n",
        "        self.expand3 = ExpandingBlock(hidden_channels * 8, hidden_channels * 4, time_emb_dim)\n",
        "        self.expand4 = ExpandingBlock(hidden_channels * 4, hidden_channels * 2, time_emb_dim)\n",
        "        self.expand5 = ExpandingBlock(hidden_channels * 2, hidden_channels, time_emb_dim)\n",
        "\n",
        "\n",
        "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        time_emb = self.time_mlp(time)\n",
        "\n",
        "\n",
        "        x0 = self.upfeature(x)\n",
        "        x1 = self.contract1(x0, time_emb)\n",
        "        x2 = self.contract2(x1, time_emb)\n",
        "        x3 = self.contract3(x2, time_emb)\n",
        "        x4 = self.contract4(x3, time_emb)\n",
        "        x5 = self.contract5(x4, time_emb)\n",
        "        x6 = self.contract6(x5, time_emb)\n",
        "\n",
        "        x7 = self.expand0(x6, x5, time_emb)\n",
        "        x8 = self.expand1(x7, x4, time_emb)\n",
        "        x9 = self.expand2(x8, x3, time_emb)\n",
        "        x10 = self.expand3(x9, x2, time_emb)\n",
        "        x11 = self.expand4(x10, x1, time_emb)\n",
        "        x12 = self.expand5(x11, x0, time_emb)\n",
        "\n",
        "        xn = self.downfeature(x12)\n",
        "\n",
        "        return xn\n",
        "\n",
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = (X + 1) / 2\n",
        "        X = self.normalize(X)\n",
        "\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self,layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrhN8a16DIVO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_latents(images):\n",
        "    # images: [B,3,H,W], range [-1,1]\n",
        "    with torch.no_grad():\n",
        "        latents = vae.encode(images).latent_dist.sample()\n",
        "        latents = latents * 0.18215  # SD scaling factor\n",
        "    return latents\n",
        "\n",
        "def decode_latents(latents):\n",
        "    latents = latents / 0.18215\n",
        "    with torch.no_grad():\n",
        "        imgs = vae.decode(latents).sample\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45WWihWrDVPC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 0.00001\n",
        "mse_loss = nn.MSELoss().to(device)\n",
        "vgg_loss = VGGLoss().to(device)\n",
        "vgg_lambda = 0.001\n",
        "epochs = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcECTan6DEKh"
      },
      "outputs": [],
      "source": [
        "# Load pretrained Stable Diffusion VAE\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    subfolder=\"vae\"\n",
        ").to(device)\n",
        "vae.eval()  # keep frozen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihqf163iXyQO"
      },
      "outputs": [],
      "source": [
        "# path = os.listdir(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/cloth/\")\n",
        "\n",
        "# inputs_viton = []\n",
        "# original_images = []\n",
        "# mask_images = []\n",
        "\n",
        "# for img in path:\n",
        "\n",
        "#     input_viton = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/inputs_difussion_model/\" + img).to(device)\n",
        "#     original_image = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/images/\" + img).to(device)\n",
        "#     mask_image = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/agnostic_mask/\" + img).to(device)\n",
        "\n",
        "#     inputs_viton.append(input_viton)\n",
        "#     original_images.append(original_image)\n",
        "#     mask_images.append(mask_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HloQbLIHZxNW"
      },
      "outputs": [],
      "source": [
        "original_images = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/images_sample_2.pth').to(device)\n",
        "inputs_viton = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/input_difussion_model_sample.pth').to(device)\n",
        "mask_images = torch.load('/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/train/torch_images/agnostic_masks_sample_2.pth').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G1WXy9bP3YH"
      },
      "outputs": [],
      "source": [
        "original_images = torch.unbind(original_images, dim=0)\n",
        "inputs_viton = torch.unbind(inputs_viton, dim=0)\n",
        "mask_images = torch.unbind(mask_images, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNqH-z5sPVg7"
      },
      "outputs": [],
      "source": [
        "original_images = [i.unsqueeze(0) for i in original_images]\n",
        "inputs_viton = [i.unsqueeze(0) for i in inputs_viton]\n",
        "mask_images = [i.unsqueeze(0) for i in mask_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoguK18NC67x"
      },
      "outputs": [],
      "source": [
        "viton = UNet(9, 4).to(device)\n",
        "viton_opt = torch.optim.AdamW(viton.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    if isinstance(m, nn.GroupNorm):\n",
        "        if hasattr(m, 'weight') and m.weight is not None:\n",
        "            torch.nn.init.constant_(m.weight, 1)\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "viton = viton.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHtafTaZQji2"
      },
      "outputs": [],
      "source": [
        "#lrscheduler = lr_scheduler.CosineAnnealingLR(viton_opt, T_max=epochs, eta_min=1e-8) # A very small min LR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH ='/content/drive/MyDrive/AIClothes/DDPM/Models/Diffusion/latent512_1500img_resnet_viton_adamW_schedulelr1e-05_vgg0.001_epoch_380.pth'\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(device))\n",
        "\n",
        "viton.load_state_dict(checkpoint['model_state_dict'])\n",
        "viton_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "losses = checkpoint['loss']"
      ],
      "metadata": {
        "id": "FanEyL98RpKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFk3gNosrQrw"
      },
      "outputs": [],
      "source": [
        "scheduler = NoiseScheduler(timesteps=1000, beta_schedule=\"linear\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7We5JSdIS7v"
      },
      "outputs": [],
      "source": [
        "z_original_images = []\n",
        "z_inputs_viton = []\n",
        "z_mask_images = []\n",
        "\n",
        "for original_image, input_viton, mask_image in zip(original_images, inputs_viton, mask_images):\n",
        "    z_original_image = encode_latents(original_image)\n",
        "    z_input_viton = encode_latents(input_viton)\n",
        "    z_mask_image = F.interpolate(mask_image, size=z_original_image.shape[-2:], mode=\"nearest\")\n",
        "\n",
        "    z_original_images.append(z_original_image)\n",
        "    z_inputs_viton.append(z_input_viton)\n",
        "    z_mask_images.append(z_mask_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWpkkkW9CjcP"
      },
      "outputs": [],
      "source": [
        "#losses = []\n",
        "\n",
        "for epoch in range(381, epochs+1):\n",
        "\n",
        "  for original_image, z_original_image, input_viton, z_input_viton, mask_image, z_mask_image in zip(original_images, z_original_images, inputs_viton, z_inputs_viton, mask_images, z_mask_images):\n",
        "\n",
        "    x_0_original = original_image\n",
        "\n",
        "    t = torch.randint(0, scheduler.timesteps, (1, )).to(device)\n",
        "\n",
        "    z_t_original, noise = scheduler.get_noisy_image(z_original_image, t)\n",
        "\n",
        "    x_0_agnostic = input_viton\n",
        "\n",
        "    z_t_agnostic, _ = scheduler.get_noisy_image(z_input_viton, t, noise)\n",
        "\n",
        "    input_original = torch.concat([z_t_original, z_input_viton, z_mask_image[:, 0:1, :, :]], dim=1)\n",
        "    input_agnostic = torch.concat([z_t_agnostic, z_input_viton, z_mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "    unet_output_original = viton(input_original, torch.tensor([t]).to(device))\n",
        "\n",
        "    unet_output_agnostic = viton(input_agnostic, torch.tensor([t]).to(device))\n",
        "\n",
        "    z_denoise_agnostic = scheduler.denoise_image(z_t_agnostic, t, unet_output_agnostic)\n",
        "\n",
        "    denoise_agnostic = decode_latents(z_denoise_agnostic)\n",
        "\n",
        "    loss = mse_loss(unet_output_original, noise) + (vgg_lambda * vgg_loss(denoise_agnostic, x_0_original))\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    viton_opt.zero_grad()\n",
        "    loss.backward()\n",
        "    viton_opt.step()\n",
        "\n",
        "    if epoch == 381:\n",
        "\n",
        "      print(f\"Epoch {epoch}, Mean Loss: {np.mean(losses)} at timestep {t} and lr: {lr}\")\n",
        "\n",
        "      fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
        "      axs[0].imshow(((original_image[0]).permute(1, 2, 0).detach().cpu().numpy()+1)/2)\n",
        "      axs[0].axis('off')\n",
        "\n",
        "      # axs[1].imshow((x_t_original[0]).permute(1, 2, 0).detach().cpu().numpy())\n",
        "      # axs[1].axis('off')\n",
        "\n",
        "      axs[2].imshow(((denoise_agnostic[0]).permute(1, 2, 0).detach().cpu().numpy()+1)/2)\n",
        "      axs[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': viton.state_dict(),\n",
        "        'optimizer_state_dict': viton_opt.state_dict(),\n",
        "        'loss': losses,\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/DDPM/Models/Diffusion/latent512_1500img_resnet_viton_adamW_schedulelr{lr}_vgg{vgg_lambda}_epoch_{epoch}.pth')\n",
        "\n",
        "  print(f\"Epoch {epoch}, Mean Loss: {np.mean(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXhGWRQs4NMB"
      },
      "outputs": [],
      "source": [
        "# Save the model checkpoint\n",
        "checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': viton.state_dict(),\n",
        "    'optimizer_state_dict': viton_opt.state_dict(),\n",
        "    'loss': losses,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/DDPM/Models/Diffusion/latent512_1500img_resnet_viton_adamW_schedulelr{lr}_vgg{vgg_lambda}_epoch_{epoch-1}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM8wHpjsqdoh"
      },
      "outputs": [],
      "source": [
        "path = os.listdir(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/inputs_difussion_model/\")\n",
        "\n",
        "for img in path:\n",
        "\n",
        "    test_input_viton = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/inputs_difussion_model/\" + img).to(device)\n",
        "    #original_image = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/images/\" + img).to(device)\n",
        "    test_mask_image = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/agnostic_mask/\" + img).to(device)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o5HQZLIsCM_"
      },
      "outputs": [],
      "source": [
        "plt.imshow((test_input_viton[0].permute(1, 2, 0).cpu()+1)/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGNGw1Qd-drv"
      },
      "outputs": [],
      "source": [
        "z_test_input_viton = encode_latents(test_input_viton)\n",
        "z_test_mask_image = F.interpolate(test_mask_image, size=z_test_input_viton.shape[-2:], mode=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mbw1y3EA4F4"
      },
      "outputs": [],
      "source": [
        "z_test_input_viton.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps_to_sample = torch.linspace(scheduler.timesteps - 1, 0, 200).to(device).long()\n",
        "x_current, _ = scheduler.get_noisy_image(z_test_input_viton, timesteps_to_sample[0])\n",
        "\n",
        "final_image = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, t in enumerate(timesteps_to_sample):\n",
        "        t_prev = timesteps_to_sample[i + 1] if i < len(timesteps_to_sample) - 1 else 0\n",
        "\n",
        "        input_to_unet = torch.cat([x_current, z_test_input_viton, z_test_mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "        predicted_noise = viton(input_to_unet, torch.tensor([t]).to(device))\n",
        "\n",
        "        alpha_bar_t_val = scheduler.alphas_cumprod[t].item()\n",
        "        x_0_pred = (x_current - math.sqrt(1.0 - alpha_bar_t_val) * predicted_noise) / math.sqrt(alpha_bar_t_val)\n",
        "\n",
        "        #x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "        alpha_bar_t_prev_val = scheduler.alphas_cumprod[t_prev].item()\n",
        "        x_current = math.sqrt(alpha_bar_t_prev_val) * x_0_pred + math.sqrt(1.0 - alpha_bar_t_prev_val) * predicted_noise\n",
        "\n",
        "        #x_current = torch.clamp(x_current, -1.0, 1.0)\n",
        "\n",
        "        x_0_pred = decode_latents(x_0_pred)\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "\n",
        "          plt.figure(figsize=(3, 3))\n",
        "          plt.imshow((x_0_pred[0].permute(1, 2, 0).cpu().numpy() + 1)/2)\n",
        "          plt.title(f\"DDIM Step {i+1} (t={t.item()})\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "final_image.append(x_0_pred)"
      ],
      "metadata": {
        "id": "-sbo2jPkQ1Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps_to_sample = torch.linspace(scheduler.timesteps - 1, 0, 200).to(device).long()\n",
        "x_current, _ = scheduler.get_noisy_image(z_test_input_viton, timesteps_to_sample[0])\n",
        "\n",
        "final_image = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, t in enumerate(timesteps_to_sample):\n",
        "        t_prev = timesteps_to_sample[i + 1] if i < len(timesteps_to_sample) - 1 else 0\n",
        "\n",
        "        input_to_unet = torch.cat([x_current, z_test_input_viton, z_test_mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "        predicted_noise = viton(input_to_unet, torch.tensor([t]).to(device))\n",
        "\n",
        "        alpha_bar_t_val = scheduler.alphas_cumprod[t].item()\n",
        "        x_0_pred = (x_current - math.sqrt(1.0 - alpha_bar_t_val) * predicted_noise) / math.sqrt(alpha_bar_t_val)\n",
        "\n",
        "        #x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "        alpha_bar_t_prev_val = scheduler.alphas_cumprod[t_prev].item()\n",
        "        x_current = math.sqrt(alpha_bar_t_prev_val) * x_0_pred + math.sqrt(1.0 - alpha_bar_t_prev_val) * predicted_noise\n",
        "\n",
        "        #x_current = torch.clamp(x_current, -1.0, 1.0)\n",
        "\n",
        "        x_0_pred = decode_latents(x_0_pred)\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "\n",
        "          plt.figure(figsize=(3, 3))\n",
        "          plt.imshow((x_0_pred[0].permute(1, 2, 0).cpu().numpy() + 1)/2)\n",
        "          plt.title(f\"DDIM Step {i+1} (t={t.item()})\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "final_image.append(x_0_pred)"
      ],
      "metadata": {
        "id": "DRemfWtULYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps_to_sample = torch.linspace(scheduler.timesteps - 1, 0, 200).to(device).long()\n",
        "x_current, _ = scheduler.get_noisy_image(z_test_input_viton, timesteps_to_sample[0])\n",
        "\n",
        "final_image = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, t in enumerate(timesteps_to_sample):\n",
        "        t_prev = timesteps_to_sample[i + 1] if i < len(timesteps_to_sample) - 1 else 0\n",
        "\n",
        "        input_to_unet = torch.cat([x_current, z_test_input_viton, z_test_mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "        predicted_noise = viton(input_to_unet, torch.tensor([t]).to(device))\n",
        "\n",
        "        alpha_bar_t_val = scheduler.alphas_cumprod[t].item()\n",
        "        x_0_pred = (x_current - math.sqrt(1.0 - alpha_bar_t_val) * predicted_noise) / math.sqrt(alpha_bar_t_val)\n",
        "\n",
        "        #x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "        alpha_bar_t_prev_val = scheduler.alphas_cumprod[t_prev].item()\n",
        "        x_current = math.sqrt(alpha_bar_t_prev_val) * x_0_pred + math.sqrt(1.0 - alpha_bar_t_prev_val) * predicted_noise\n",
        "\n",
        "        #x_current = torch.clamp(x_current, -1.0, 1.0)\n",
        "\n",
        "        x_0_pred = decode_latents(x_0_pred)\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "\n",
        "          plt.figure(figsize=(3, 3))\n",
        "          plt.imshow((x_0_pred[0].permute(1, 2, 0).cpu().numpy() + 1)/2)\n",
        "          plt.title(f\"DDIM Step {i+1} (t={t.item()})\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "final_image.append(x_0_pred)"
      ],
      "metadata": {
        "id": "4sInZA85kSdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qI7EnOeCmrXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}