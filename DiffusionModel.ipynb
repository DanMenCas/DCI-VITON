{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLfV4UqLlRgM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN1NqjeGV55e"
      },
      "outputs": [],
      "source": [
        "size = (256, 256)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(size),        # Resize to fixed size\n",
        "        transforms.ToTensor(),          # Converts to [C, H, W], values in [0, 1]\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)  # Optional if using Tanh output\n",
        "    ])\n",
        "\n",
        "def load_image(image_path, transform=transform):\n",
        "    \"\"\"\n",
        "    Loads a single image and converts it to a tensor of shape [1, 3, H, W]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
        "    image = transform(image)                      # [3, H, W]\n",
        "\n",
        "    tensor = image.unsqueeze(0)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7W8dgmON08i"
      },
      "outputs": [],
      "source": [
        "class NoiseScheduler:\n",
        "    def __init__(self, timesteps=1000, beta_schedule=\"linear\", device='cpu'):\n",
        "        self.timesteps = timesteps\n",
        "        self.device = torch.device(device) # Store the device\n",
        "\n",
        "        if beta_schedule == \"linear\":\n",
        "            betas = self._linear_beta_schedule(timesteps)\n",
        "        elif beta_schedule == \"cosine\":\n",
        "            betas = self._cosine_beta_schedule(timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown beta schedule: {beta_schedule}\")\n",
        "\n",
        "        # Move all pre-computed tensors to the specified device\n",
        "        self.betas = betas.to(self.device)\n",
        "        self.alphas = (1.0 - self.betas).to(self.device)\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(self.device) # This is alpha_bar_t\n",
        "\n",
        "        # Pre-compute square roots for convenience in the forward process\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(self.device)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod).to(self.device)\n",
        "\n",
        "        # Ensure these are also on the correct device\n",
        "        # You might also want to pre-compute these if you use them frequently to avoid repeated indexing\n",
        "        # self.sqrt_recip_alphas = (1.0 / torch.sqrt(self.alphas)).to(self.device)\n",
        "        # self.posterior_variance = ... (for sampling)\n",
        "\n",
        "    def _linear_beta_schedule(self, timesteps, start=0.0001, end=0.02):\n",
        "        # Betas are created on CPU, will be moved to self.device in __init__\n",
        "        return torch.linspace(start, end, timesteps, dtype=torch.float32)\n",
        "\n",
        "    def _cosine_beta_schedule(self, timesteps, s=0.008):\n",
        "        # Betas are created on CPU, will be moved to self.device in __init__\n",
        "        t = torch.arange(timesteps + 1, dtype=torch.float32)\n",
        "        f_t = torch.cos(((t / timesteps) + s) / (1 + s) * math.pi / 2) ** 2\n",
        "        alphas_bar = f_t / f_t[0]\n",
        "        betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
        "        return torch.clip(betas, 0.0001, 0.999) # Clip for stability\n",
        "\n",
        "    def get_noisy_image(self, x_0, t, noise=None):\n",
        "        \"\"\"\n",
        "        Adds noise to the original image x_0 at time t.\n",
        "        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
        "        \"\"\"\n",
        "        # Ensure 'noise' is on the same device as x_0\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0, device=self.device) # Create noise directly on the device\n",
        "            # Alternatively: noise = torch.randn_like(x_0).to(self.device)\n",
        "\n",
        "        # The 't' tensor should already be on the correct device (passed from the training loop).\n",
        "        # We need to ensure that the alpha_bar_t value indexed from self.alphas_cumprod\n",
        "        # has the correct shape for broadcasting and is on the same device as x_0.\n",
        "        # self.alphas_cumprod is already on `self.device`\n",
        "\n",
        "        # Ensure t has the correct shape for broadcasting\n",
        "        # If t is a batch of scalar integers, expand its dimensions\n",
        "        if t.ndim == 1: # (batch_size,)\n",
        "            # Expand to (batch_size, 1, 1, 1) for image dimensions\n",
        "            alpha_bar_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        else: # Assuming t is already shaped correctly, e.g., (1, 1, 1, 1) or scalar\n",
        "            # This case means t might already have its own batch dimension\n",
        "            # Ensure it's on the correct device for indexing (though it should be by now)\n",
        "            # and then get the value.\n",
        "            alpha_bar_t = self.alphas_cumprod[t]\n",
        "            # No need for alpha_bar_t.to(x_0.device) because self.alphas_cumprod is already on self.device\n",
        "\n",
        "        # If t was accidentally on a different device (e.g., cpu) when passed in,\n",
        "        # self.alphas_cumprod[t] would automatically result in a tensor on self.device\n",
        "        # if self.alphas_cumprod is on self.device. PyTorch handles this cross-device indexing,\n",
        "        # but it's more efficient if 't' is already on the target device.\n",
        "\n",
        "        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
        "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)\n",
        "\n",
        "        x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n",
        "        return x_t, noise # Return x_t and the noise that was added (for loss calculation)\n",
        "\n",
        "    def denoise_image(self, x_t, t, noise_unet):\n",
        "        \"\"\"\n",
        "        Denoises an image x_t given the predicted noise (noise_unet) at time t.\n",
        "        This is the reverse step.\n",
        "        \"\"\"\n",
        "        # Similar logic as get_noisy_image for handling 't' and ensuring device consistency\n",
        "        if t.ndim == 1: # (batch_size,)\n",
        "            alpha_bar_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        else:\n",
        "            alpha_bar_t = self.alphas_cumprod[t]\n",
        "\n",
        "        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
        "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)\n",
        "\n",
        "        # All tensors involved in this calculation (x_t, noise_unet, sqrt_alpha_bar_t, etc.)\n",
        "        # must be on the same device. x_t and noise_unet are assumed to be on the target device\n",
        "        # from the model's output and input.\n",
        "        x_0 = (x_t - sqrt_one_minus_alpha_bar_t * noise_unet) / sqrt_alpha_bar_t\n",
        "\n",
        "        return x_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwdfRbIsOa_J"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal Positional Embedding for time steps.\n",
        "    Transforms a scalar time step 't' into a high-dimensional vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.half_dim = dim // 2\n",
        "        # Frequencies are based on 10000^(2i/dim)\n",
        "        self.embeddings = math.log(10000) / (self.half_dim - 1)\n",
        "        self.embeddings = torch.exp(torch.arange(self.half_dim) * -self.embeddings)\n",
        "\n",
        "    def forward(self, time):\n",
        "        # time is typically a batch of scalars (B,)\n",
        "        # unsqueeze for broadcasting: (B, 1)\n",
        "        # multiply by frequencies: (B, half_dim)\n",
        "        time_embedding = time.unsqueeze(1) * self.embeddings.to(time.device)\n",
        "\n",
        "        # Apply sine and cosine: (B, half_dim) -> (B, dim)\n",
        "        time_embedding = torch.cat((time_embedding.sin(), time_embedding.cos()), dim=-1)\n",
        "        return time_embedding\n",
        "\n",
        "class TimeEmbeddingMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP to process the sinusoidal time embedding into scale and shift parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.SiLU(), # SiLU is a common activation in modern diffusion models\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wii3V9viNsv-"
      },
      "outputs": [],
      "source": [
        "class AdaptiveGroupNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Group Normalization (AdaGN) layer.\n",
        "    Applies GroupNorm, then scales and shifts features based on conditioning (time embedding).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_groups, num_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(num_groups, num_channels, affine=False) # affine=False means no learnable scale/shift in GroupNorm itself\n",
        "        self.time_proj = nn.Linear(time_emb_dim, 2 * num_channels) # Projects time_emb to 2 * num_channels (for scale and shift)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # x: (B, C, H, W)\n",
        "        # time_emb: (B, time_emb_dim)\n",
        "\n",
        "        # Apply GroupNorm first\n",
        "        normed_x = self.norm(x)\n",
        "\n",
        "        # Get scale and shift from time embedding\n",
        "        scale_shift = self.time_proj(time_emb) # (B, 2 * num_channels)\n",
        "        scale, shift = scale_shift.chunk(2, dim=1) # (B, num_channels), (B, num_channels)\n",
        "\n",
        "        # Reshape scale and shift for broadcasting: (B, C, 1, 1)\n",
        "        scale = scale.unsqueeze(-1).unsqueeze(-1)\n",
        "        shift = shift.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Apply adaptive scaling and shifting\n",
        "        output = normed_x * (1 + scale) + shift # The `1 + scale` ensures that if scale is 0, it doesn't zero out the features.\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN6b1VTLNyPn"
      },
      "outputs": [],
      "source": [
        "class ContractingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, use_dropout=False): # Added time_emb_dim\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) # Changed stride=2 to padding=1, and will use another conv for downsample\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.norm1 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim) # Use AdaGN\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim) # Use AdaGN\n",
        "\n",
        "        self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1) # Explicit downsample after the convs\n",
        "\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3) # Common dropout rate\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "    def forward(self, x, time_emb): # Now accepts time_emb\n",
        "        skip_x = x # Save x for residual connection (common in diffusion U-Nets)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, time_emb) # Pass time_emb to AdaGN\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, time_emb) # Pass time_emb to AdaGN\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Optional: Add a residual connection before downsampling\n",
        "        # x = x + skip_x # This requires skip_x to have same shape. If not, it's a ResNet-style block where first conv changes channels.\n",
        "        # For typical diffusion blocks, you'd apply the initial convolution with proper input/output channels to make this work.\n",
        "        # Let's keep it simpler for now and assume typical residual blocks where output channels match input channels for the shortcut.\n",
        "\n",
        "        # Downsample\n",
        "        x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "class ExpandingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, use_dropout=False): # Added time_emb_dim\n",
        "        super().__init__()\n",
        "        # Changed upsample: Now upsamples to `out_channels` (which will be `skip_con_x` channels).\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        # Convs after concatenation (input channels will be doubled due to skip connection)\n",
        "        self.conv1 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = AdaptiveGroupNorm(num_groups=8, num_channels=out_channels, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        self.activation = nn.ReLU() # ReLU is fine here\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "    def forward(self, x, skip_con_x, time_emb): # Now accepts time_emb\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        # Pad or crop skip_con_x if dimensions don't match exactly after upsampling\n",
        "        # This is common if strides/paddings lead to slight discrepancies.\n",
        "        # For simplicity, assuming perfect alignment for now based on kernel=2, stride=2 upsample\n",
        "        if x.shape != skip_con_x.shape:\n",
        "             x = torch.nn.functional.interpolate(x, size=skip_con_x.shape[2:], mode='nearest') # Or use F.pad for padding smaller x.\n",
        "\n",
        "        x = torch.cat([x, skip_con_x], dim=1) # dim=1 for channel concatenation\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, time_emb) # Pass time_emb to AdaGN\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, time_emb) # Pass time_emb to AdaGN\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# FeatureMapBlock remains the same, as it's just input/output mapping\n",
        "class FeatureMapBlock(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjYudbrBN3oa"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, hidden_channels=32, time_emb_dim=256): # Added time_emb_dim\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding modules\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionalEmbedding(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 4), # A common design choice is to expand the time emb size\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim * 4, time_emb_dim) # Then bring it back to a standard size for injection\n",
        "        )\n",
        "        self.time_emb_dim = time_emb_dim # Store it for internal use\n",
        "\n",
        "        # Initial feature mapping\n",
        "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
        "\n",
        "        # Contracting Path\n",
        "        self.contract1 = ContractingBlock(hidden_channels, hidden_channels * 2, time_emb_dim)\n",
        "        self.contract2 = ContractingBlock(hidden_channels * 2, hidden_channels * 4, time_emb_dim)\n",
        "        self.contract3 = ContractingBlock(hidden_channels * 4, hidden_channels * 8, time_emb_dim)\n",
        "        self.contract4 = ContractingBlock(hidden_channels * 8, hidden_channels * 16, time_emb_dim)\n",
        "        self.contract5 = ContractingBlock(hidden_channels * 16, hidden_channels * 32, time_emb_dim)\n",
        "        self.contract6 = ContractingBlock(hidden_channels * 32, hidden_channels * 64, time_emb_dim)\n",
        "\n",
        "        # Expanding Path\n",
        "        # Note: The output channels of ExpandingBlock's upsample should match the skip connection's channels.\n",
        "        # And the convs inside ExpandingBlock will manage their own `out_channels` (which is half of the *incoming* features after upsample+concat).\n",
        "        # Let's define the `out_channels` for each ExpandingBlock to match the `skip_con_x` channels they're connecting to.\n",
        "        self.expand0 = ExpandingBlock(hidden_channels * 64, hidden_channels * 32, time_emb_dim) # upsamples 64->32, concat with x5 (32), then convs work on 64\n",
        "        self.expand1 = ExpandingBlock(hidden_channels * 32, hidden_channels * 16, time_emb_dim)\n",
        "        self.expand2 = ExpandingBlock(hidden_channels * 16, hidden_channels * 8, time_emb_dim)\n",
        "        self.expand3 = ExpandingBlock(hidden_channels * 8, hidden_channels * 4, time_emb_dim)\n",
        "        self.expand4 = ExpandingBlock(hidden_channels * 4, hidden_channels * 2, time_emb_dim)\n",
        "        self.expand5 = ExpandingBlock(hidden_channels * 2, hidden_channels, time_emb_dim) # Final expand block outputting to original hidden_channels\n",
        "\n",
        "        # Final output layer\n",
        "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
        "        # self.sigmoid = torch.nn.Sigmoid() # REMOVE THIS FOR NOISE PREDICTION\n",
        "\n",
        "    def forward(self, x, time): # NOW TAKES TIME AS INPUT!\n",
        "        # Process time embedding\n",
        "        time_emb = self.time_mlp(time)\n",
        "\n",
        "        # Contracting Path\n",
        "        x0 = self.upfeature(x) # hidden_channels\n",
        "        x1 = self.contract1(x0, time_emb) # hidden_channels * 2\n",
        "        x2 = self.contract2(x1, time_emb) # hidden_channels * 4\n",
        "        x3 = self.contract3(x2, time_emb) # hidden_channels * 8\n",
        "        x4 = self.contract4(x3, time_emb) # hidden_channels * 16\n",
        "        x5 = self.contract5(x4, time_emb) # hidden_channels * 32\n",
        "        x6 = self.contract6(x5, time_emb) # hidden_channels * 64 (bottleneck)\n",
        "\n",
        "        # Expanding Path\n",
        "        x7 = self.expand0(x6, x5, time_emb) # hidden_channels * 32\n",
        "        x8 = self.expand1(x7, x4, time_emb) # hidden_channels * 16\n",
        "        x9 = self.expand2(x8, x3, time_emb) # hidden_channels * 8\n",
        "        x10 = self.expand3(x9, x2, time_emb) # hidden_channels * 4\n",
        "        x11 = self.expand4(x10, x1, time_emb) # hidden_channels * 2\n",
        "        x12 = self.expand5(x11, x0, time_emb) # hidden_channels (back to original input channels)\n",
        "\n",
        "        # Final output convolution\n",
        "        xn = self.downfeature(x12)\n",
        "\n",
        "        return xn # Return raw output (noise prediction)\n",
        "\n",
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = (X + 1) / 2\n",
        "        X = self.normalize(X)\n",
        "\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self,layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 0.00001\n",
        "mse_loss = nn.MSELoss().to(device)\n",
        "vgg_loss = VGGLoss().to(device)\n",
        "vgg_lambda = 0.001\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "IdOtP-jIOhnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihqf163iXyQO"
      },
      "outputs": [],
      "source": [
        "path = os.listdir(\"/content/drive/MyDrive/AIClothes/Inputs_VITON/inputs_difussion_model/\")\n",
        "\n",
        "inputs_viton = []\n",
        "original_images = []\n",
        "mask_images = []\n",
        "\n",
        "for img in path:\n",
        "\n",
        "    input_viton = load_image(\"/content/drive/MyDrive/AIClothes/Inputs_VITON/inputs_difussion_model/\" + img).to(device)\n",
        "    original_image = load_image(\"/content/drive/MyDrive/AIClothes/Inputs_VITON/images/\" + img).to(device)\n",
        "    mask_image = load_image(\"/content/drive/MyDrive/AIClothes/Inputs_VITON/agnostic_mask/\" + img).to(device)\n",
        "\n",
        "    inputs_viton.append(input_viton)\n",
        "    original_images.append(original_image)\n",
        "    mask_images.append(mask_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoguK18NC67x"
      },
      "outputs": [],
      "source": [
        "viton = UNet(7, 3).to(device)\n",
        "viton_opt = torch.optim.AdamW(viton.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_normal_(m.weight) # Good for conv layers\n",
        "        if m.bias is not None: # Initialize bias if it exists\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "    # If you are using GroupNorm (which is part of AdaptiveGroupNorm often):\n",
        "    if isinstance(m, nn.GroupNorm):\n",
        "        if hasattr(m, 'weight') and m.weight is not None:\n",
        "            torch.nn.init.constant_(m.weight, 1) # Gamma\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "viton = viton.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lrscheduler = lr_scheduler.CosineAnnealingLR(viton_opt, T_max=epochs, eta_min=1e-8) # A very small min LR"
      ],
      "metadata": {
        "id": "DHtafTaZQji2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVKCi0CB4DQ5"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH ='/content/drive/MyDrive/AIClothes/Models/Diffusion/allimg_viton_adamW_schedulelr1e-05_vgg0.001_epoch_499.pth'\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "\n",
        "viton.load_state_dict(checkpoint['model_state_dict'])\n",
        "viton_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "losses = checkpoint['loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWpkkkW9CjcP"
      },
      "outputs": [],
      "source": [
        "scheduler = NoiseScheduler(timesteps=1000, beta_schedule=\"linear\", device=device)\n",
        "\n",
        "#losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for original_image, input_viton, mask_image in zip(original_images, inputs_viton, mask_images):\n",
        "\n",
        "    x_0_original = original_image\n",
        "    t = torch.randint(0, scheduler.timesteps, (1, )).to(device) # Random batch of timesteps\n",
        "    x_t_original, noise = scheduler.get_noisy_image(x_0_original, t)\n",
        "\n",
        "    x_0_agnostic = input_viton\n",
        "    x_t_agnostic, _ = scheduler.get_noisy_image(x_0_agnostic, t, noise)\n",
        "\n",
        "    input_original = torch.concat([x_t_original, input_viton, mask_image[:, 0:1, :, :]], dim=1)\n",
        "    input_agnostic = torch.concat([x_t_agnostic, input_viton, mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "    unet_output_original = viton(input_original, torch.tensor([t]).to(device))\n",
        "\n",
        "    unet_output_agnostic = viton(input_agnostic, torch.tensor([t]).to(device))\n",
        "\n",
        "    denoise_agnostic = scheduler.denoise_image(x_t_agnostic, t, unet_output_agnostic)\n",
        "\n",
        "    loss = mse_loss(unet_output_original, noise) + (vgg_lambda * vgg_loss(denoise_agnostic, x_0_original))\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    viton_opt.zero_grad()\n",
        "    loss.backward()\n",
        "    viton_opt.step()\n",
        "\n",
        "    #lrscheduler.step()\n",
        "\n",
        "    # Log current LR to see its progression\n",
        "    current_lr = viton_opt.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    if epoch == epochs-1:\n",
        "\n",
        "      print(f\"Epoch {epoch}, Mean Loss: {np.mean(losses)} at timestep {t} and lr: {current_lr}\")\n",
        "\n",
        "      fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
        "      axs[0].imshow(((original_image[0]+1) / 2).permute(1, 2, 0).detach().cpu().numpy())\n",
        "      axs[0].axis('off')\n",
        "\n",
        "      axs[1].imshow(((x_t_original[0]+1)/2).permute(1, 2, 0).detach().cpu().numpy())\n",
        "      axs[1].axis('off')\n",
        "\n",
        "      axs[2].imshow(((denoise_agnostic[0]+1)/2).permute(1, 2, 0).detach().cpu().numpy())\n",
        "      axs[2].axis('off')\n",
        "\n",
        "    #   plt.show()\n",
        "\n",
        "    # if count % 500 == 0:\n",
        "\n",
        "    #   checkpoint = {\n",
        "    #       'epoch': epoch,\n",
        "    #       'model_state_dict': viton.state_dict(),\n",
        "    #       'optimizer_state_dict': viton_opt.state_dict(),\n",
        "    #       'loss': losses,\n",
        "    #       # You can add more training parameters here if needed, e.g., scheduler state, random seeds\n",
        "    #   }\n",
        "\n",
        "    #   torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/Models/viton_adamW_schedulelr{lr}_vgg{vgg_lambda}_epoch_{3000+epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': viton.state_dict(),\n",
        "        'optimizer_state_dict': viton_opt.state_dict(),\n",
        "        'loss': losses,\n",
        "        # You can add more training parameters here if needed, e.g., scheduler state, random seeds\n",
        "    }\n",
        "\n",
        "torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/Models/allimg_viton_adamW_schedulelr{lr}_vgg{vgg_lambda}_epoch_{500+epoch}.pth')"
      ],
      "metadata": {
        "id": "L1KFFmaMJOIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKPOINT_PATH ='/content/drive/MyDrive/AIClothes/Models/viton_adamW_schedulelr0.0001_vgg0.001_epoch_2999.pth'\n",
        "\n",
        "# checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "\n",
        "# viton.load_state_dict(checkpoint['model_state_dict'])\n",
        "# viton_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# losses = checkpoint['loss']"
      ],
      "metadata": {
        "id": "bl5v7UyVG242"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps_to_sample = torch.linspace(scheduler.timesteps - 10, 5, 100).to(device).long()\n",
        "x_current, _ = scheduler.get_noisy_image(input_viton, timesteps_to_sample[0])\n",
        "\n",
        "final_image = []\n",
        "\n",
        "with torch.no_grad():\n",
        "# --- DDIM Denoising Loop ---\n",
        "    for i, t in enumerate(timesteps_to_sample):\n",
        "        # Determine the previous timestep in your custom sequence\n",
        "        t_prev = timesteps_to_sample[i + 1] if i < len(timesteps_to_sample) - 1 else 0\n",
        "\n",
        "        # --- CRITICAL: CORRECTLY CONSTRUCT THE UNet INPUT ---\n",
        "        # input_to_unet MUST be [current_noisy_image, STATIC_WARPED_CLOTH, STATIC_WARPED_MASK]\n",
        "        input_to_unet = torch.cat([x_current, input_viton, mask_image[:, 0:1, :, :]], dim=1)\n",
        "\n",
        "        # --- Call the UNet to predict noise ---\n",
        "        # (assuming your UNet is named 'viton')\n",
        "        predicted_noise = viton(input_to_unet, torch.tensor([t]).to(device))\n",
        "\n",
        "        # --- PERFORM THE DDIM UPDATE (Explicitly, NOT using scheduler.denoise_image or get_noisy_image) ---\n",
        "\n",
        "        # 1. Estimate the clean image (x_0_pred) from the current noisy image (x_current) and predicted noise\n",
        "        alpha_bar_t_val = scheduler.alphas_cumprod[t].item()\n",
        "        x_0_pred = (x_current - math.sqrt(1.0 - alpha_bar_t_val) * predicted_noise) / math.sqrt(alpha_bar_t_val)\n",
        "\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0) # <--- ESSENTIAL CLAMPING\n",
        "\n",
        "        # # 2. Calculate the next noisy state (x_{t-1}) using x_0_pred and predicted_noise\n",
        "        alpha_bar_t_prev_val = scheduler.alphas_cumprod[t_prev].item()\n",
        "        x_current = math.sqrt(alpha_bar_t_prev_val) * x_0_pred + math.sqrt(1.0 - alpha_bar_t_prev_val) * predicted_noise\n",
        "        x_current = torch.clamp(x_current, -1.0, 1.0) # <--- ESSENTIAL CLAMPING\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "\n",
        "          plt.imshow(((x_0_pred[0].permute(1, 2, 0).cpu().numpy()) + 1) / 2)\n",
        "          plt.title(f\"DDIM Step {i+1} (t={t.item()})\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "# After the loop, x_current holds the final denoised image\n",
        "final_image.append(x_0_pred)\n",
        "# ... (rest of your post-processing and saving/plotting code) ..."
      ],
      "metadata": {
        "id": "3GBV3Qd3jK7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evqrVD5Dzv64"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}