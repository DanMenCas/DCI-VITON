{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxuQhCA2J3vc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Voy5RwYvME"
      },
      "outputs": [],
      "source": [
        "def make_grid(N, iW, iH, device):\n",
        "    grid_x = torch.linspace(-1.0, 1.0, iW).view(1, 1, iW, 1).expand(N, iH, -1, -1).to(device)\n",
        "    grid_y = torch.linspace(-1.0, 1.0, iH).view(1, iH, 1, 1).expand(N, -1, iW, -1).to(device)\n",
        "    grid = torch.cat([grid_x, grid_y], 3)\n",
        "    return grid\n",
        "\n",
        "def save_tensor_as_image(tensor, save_path):\n",
        "    \"\"\"\n",
        "    Saves a [C, H, W] or [1, C, H, W] tensor as an image file.\n",
        "    \"\"\"\n",
        "    if tensor.dim() == 4:\n",
        "        tensor = tensor.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "    to_pil = ToPILImage()\n",
        "    image = to_pil(tensor.cpu().clamp(0, 1))  # Clamp values to [0,1] if needed\n",
        "    image.save(save_path)\n",
        "\n",
        "def flow_loss(flow_list):\n",
        "\n",
        "    loss_tv = 0\n",
        "\n",
        "    for flow in flow_list:\n",
        "      y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
        "      x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
        "      loss_tv = loss_tv + y_tv + x_tv\n",
        "\n",
        "    return loss_tv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX8og9AVYrbO"
      },
      "outputs": [],
      "source": [
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self,layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom1A4x5KmjA"
      },
      "outputs": [],
      "source": [
        "class ResNetEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels, use_dropout=False, use_bn=True, down=True, up=False):\n",
        "      super(ResNetEncoderBlock, self).__init__()\n",
        "\n",
        "      if down:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1)\n",
        "      elif up:\n",
        "          self.scale = nn.Sequential(\n",
        "              nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "              nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "          )\n",
        "      else:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
        "\n",
        "      self.activation = nn.ReLU()\n",
        "\n",
        "      if use_bn:\n",
        "          self.batchnorm = nn.InstanceNorm2d(output_channels)\n",
        "      self.use_bn = use_bn\n",
        "\n",
        "      if use_dropout:\n",
        "          self.dropout = nn.Dropout()\n",
        "      self.use_dropout = use_dropout\n",
        "\n",
        "      self.conv_1 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "      self.conv_2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      residual = self.scale(x)\n",
        "      conv1_x = self.conv_1(residual)\n",
        "      if self.use_bn:\n",
        "          conv1_x = self.batchnorm(conv1_x)\n",
        "      if self.use_dropout:\n",
        "          conv1_x = self.dropout(conv1_x)\n",
        "      conv1_x = self.activation(conv1_x)\n",
        "      conv2_x = self.conv_2(conv1_x)\n",
        "      if self.use_bn:\n",
        "          conv2_x = self.batchnorm(conv2_x)\n",
        "      if self.use_dropout:\n",
        "          conv2_x = self.dropout(conv2_x)\n",
        "\n",
        "      return self.activation(conv2_x + residual)\n",
        "\n",
        "class ClothingEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(ClothingEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "\n",
        "      return x0, x1, x2, x3, x4\n",
        "\n",
        "class SegmentEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(SegmentEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet6 = ResNetEncoderBlock(output_channels * 4, output_channels * 8, down=False)\n",
        "      self.resnet7 = ResNetEncoderBlock(output_channels * 8, output_channels * 4, down=False, up=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "      x5 = self.resnet6(x4)\n",
        "      x6 = self.resnet7(x5)\n",
        "\n",
        "      return x0, x1, x2, x3, x4, x5, x6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WarpingProcess(nn.Module):\n",
        "\n",
        "  def __init__(self, oc):\n",
        "      super(WarpingProcess, self).__init__()\n",
        "\n",
        "      self.conv_after_concat_1 = nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "\n",
        "      self.convs_clothing = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*2, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc, oc*4, kernel_size=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.conv_after_concat_2 = nn.ModuleList([\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment_after_concat = nn.ModuleList([\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*10, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*9, oc*4, down=False, up=True)\n",
        "      ])\n",
        "\n",
        "      self.clothingencoder = ClothingEncoder(4, oc)\n",
        "      self.posencoder = SegmentEncoder(6, oc)\n",
        "\n",
        "  def forward(self, input_1, input_2, device):\n",
        "\n",
        "    clothing_before_warp = []\n",
        "    pose_after_concat = []\n",
        "    clothingpose_before_warp = []\n",
        "\n",
        "    flow_list = []\n",
        "\n",
        "    ce_0, ce_1, ce_2, ce_3, ce_4 = self.clothingencoder(input_1)\n",
        "    clothing_features = [ce_0, ce_1, ce_2, ce_3, ce_4]\n",
        "\n",
        "    pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6 = self.posencoder(input_2)\n",
        "    pose_features = [pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6]\n",
        "\n",
        "    clothing_pose_last_feature = torch.cat([ce_4, pe_4], dim=1)\n",
        "\n",
        "    conv_before_warp = self.conv_after_concat_1(clothing_pose_last_feature)\n",
        "\n",
        "    for i in range(4):\n",
        "\n",
        "      if i == 0:\n",
        "\n",
        "        up = F.interpolate(clothing_features[4], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(conv_before_warp, scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_features[6], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_features[6])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "      else:\n",
        "\n",
        "        up = F.interpolate(clothing_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(clothingpose_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_after_concat[i - 1], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_after_concat[i - 1])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "    last_up = F.interpolate(clothingpose_before_warp[-1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "    grid = make_grid(1, last_up.shape[2], last_up.shape[3], device)\n",
        "\n",
        "    flow_norm = torch.cat([last_up[:, 0:1, :, :] / ((last_up.shape[3] - 1.0) / 2.0), last_up[:, 1:2, :, :] / ((last_up.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "    warped_T1 = F.grid_sample(input_1, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "    flow_list.append(flow_norm)\n",
        "\n",
        "    warped_c = warped_T1[:, :-1, :, :]\n",
        "    warped_cm = warped_T1[:, -1:, :, :]\n",
        "\n",
        "    return warped_c, warped_cm, flow_list\n"
      ],
      "metadata": {
        "id": "BaVhm1GlBbiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warped_cloth_into_agnostic(agnostic_image, warped_cloth_image, warped_cloth_mask):\n",
        "\n",
        "  agnostic_image_warped_cloth = agnostic_image.clone()\n",
        "\n",
        "  warped_cloth_mask[warped_cloth_mask > 0.5] = 1\n",
        "\n",
        "  warped_cloth_mask = warped_cloth_mask.repeat(1, 3, 1, 1)\n",
        "\n",
        "  agnostic_image_warped_cloth[warped_cloth_mask == 1] = warped_cloth_image[warped_cloth_mask == 1]\n",
        "\n",
        "  return agnostic_image_warped_cloth"
      ],
      "metadata": {
        "id": "JbeY-HpLPULs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCgvCZwc_mLw"
      },
      "outputs": [],
      "source": [
        "size = (256, 256)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(size),        # Resize to fixed size\n",
        "        transforms.ToTensor(),          # Converts to [C, H, W], values in [0, 1]\n",
        "        # transforms.Normalize([0.5]*3, [0.5]*3)  # Optional if using Tanh output\n",
        "    ])\n",
        "\n",
        "def load_image(image_path, size=size, transform=transform):\n",
        "    \"\"\"\n",
        "    Loads a single image and converts it to a tensor of shape [1, 3, H, W]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
        "    image = transform(image)                      # [3, H, W]\n",
        "\n",
        "    tensor = image.unsqueeze(0)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "g6Zk4gihkqaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Va22n5ajiv"
      },
      "outputs": [],
      "source": [
        "generator = WarpingProcess(96).to(device)\n",
        "gen_opt = torch.optim.Adam(generator.parameters(), lr=0.00005, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH ='/content/drive/MyDrive/AIClothes/DDPM/Models/Warping/3000imgs_warping_lr5e-05_vgg0.1_tvlmabda_1epoch_30.pth'\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(device))\n",
        "\n",
        "generator.load_state_dict(checkpoint['model_state_dict'])\n",
        "gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "losses = checkpoint['loss']"
      ],
      "metadata": {
        "id": "8EuAjqS4K2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs_1 = []\n",
        "test_inputs_2 = []\n",
        "\n",
        "test_cloth = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/cloth/cloth.jpg\").to(device)\n",
        "\n",
        "test_cloth_mask = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/cloth_mask/cloth_mask.jpg\").to(device)\n",
        "\n",
        "test_densepose = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/densepose/open_pose.jpg\").to(device)\n",
        "\n",
        "test_parse_agnostic = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/parse_agnostic/parse_agnostic.png\").to(device)\n",
        "\n",
        "test_agnostic = load_image(\"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/agnostic/agnostic.jpg\").to(device)\n",
        "\n",
        "test_input_1 = torch.cat([test_cloth, test_cloth_mask[:, 0:1, ...]], dim=1)\n",
        "# The parse agnostic image is multiplied by 5 for have the correct normalization.\n",
        "test_input_2 = torch.cat([test_densepose, (test_parse_agnostic)*15], dim=1)\n",
        "\n",
        "test_inputs_1.append(test_input_1)\n",
        "test_inputs_2.append(test_input_2)"
      ],
      "metadata": {
        "id": "QikZxoY68SXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_warped_c, test_warped_cm, test_flow_list = generator(test_input_1, test_input_2, device)"
      ],
      "metadata": {
        "id": "jdjxb5GPlkN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_warped_c[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efmrMcwhl41f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_im = warped_cloth_into_agnostic(test_agnostic, test_warped_c, test_warped_cm)\n",
        "\n",
        "plt.imshow(new_im[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRXs3c6MADet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_reshaped = F.interpolate(new_im, size=(512, 512), mode='bilinear', align_corners=False)"
      ],
      "metadata": {
        "id": "VREJRKPICLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im_reshaped[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ni-ruhaOASzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_tensor_as_image(im_reshaped[0], \"/content/drive/MyDrive/AIClothes/DDPM/Inputs_VITON/test/inputs_difussion_model/input_difussion_model.jpg\")"
      ],
      "metadata": {
        "id": "KHmURkjGoOjP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}