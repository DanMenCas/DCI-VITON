{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from parsehuman import Preprocessing"
      ],
      "metadata": {
        "id": "6jZTBReBsHRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxuQhCA2J3vc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import cv2\n",
        "import argparse\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from google.colab.patches import cv2_imshow\n",
        "from rembg import remove, new_session\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "from os import path as osp\n",
        "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from densepose.config import add_densepose_config\n",
        "from densepose.vis.densepose_results import DensePoseResultsFineSegmentationVisualizer\n",
        "from densepose.vis.bounding_box import ScoredBoundingBoxVisualizer\n",
        "from densepose.vis.base import CompoundVisualizer\n",
        "from densepose.vis.extractor import create_extractor, CompoundExtractor\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Voy5RwYvME"
      },
      "outputs": [],
      "source": [
        "def make_grid(N, iW, iH, device):\n",
        "    grid_x = torch.linspace(-1.0, 1.0, iW).view(1, 1, iW, 1).expand(N, iH, -1, -1).to(device)\n",
        "    grid_y = torch.linspace(-1.0, 1.0, iH).view(1, iH, 1, 1).expand(N, -1, iW, -1).to(device)\n",
        "    grid = torch.cat([grid_x, grid_y], 3)\n",
        "    return grid\n",
        "\n",
        "def save_tensor_as_image(tensor, save_path):\n",
        "    \"\"\"\n",
        "    Saves a [C, H, W] or [1, C, H, W] tensor as an image file.\n",
        "    \"\"\"\n",
        "    if tensor.dim() == 4:\n",
        "        tensor = tensor.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "    to_pil = ToPILImage()\n",
        "    image = to_pil(tensor.cpu().clamp(0, 1))  # Clamp values to [0,1] if needed\n",
        "    image.save(save_path)\n",
        "\n",
        "def flow_loss(flow_list):\n",
        "\n",
        "    loss_tv = 0\n",
        "\n",
        "    for flow in flow_list:\n",
        "      y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
        "      x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
        "      loss_tv = loss_tv + y_tv + x_tv\n",
        "\n",
        "    return loss_tv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX8og9AVYrbO"
      },
      "outputs": [],
      "source": [
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self,layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom1A4x5KmjA"
      },
      "outputs": [],
      "source": [
        "class ResNetEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels, use_dropout=False, use_bn=True, down=True, up=False):\n",
        "      super(ResNetEncoderBlock, self).__init__()\n",
        "\n",
        "      if down:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1)\n",
        "      elif up:\n",
        "          self.scale = nn.Sequential(\n",
        "              nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "              nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "          )\n",
        "      else:\n",
        "          self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
        "\n",
        "      self.activation = nn.ReLU()\n",
        "\n",
        "      if use_bn:\n",
        "          self.batchnorm = nn.InstanceNorm2d(output_channels)\n",
        "      self.use_bn = use_bn\n",
        "\n",
        "      if use_dropout:\n",
        "          self.dropout = nn.Dropout()\n",
        "      self.use_dropout = use_dropout\n",
        "\n",
        "      self.conv_1 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "      self.conv_2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      residual = self.scale(x)\n",
        "      conv1_x = self.conv_1(residual)\n",
        "      if self.use_bn:\n",
        "          conv1_x = self.batchnorm(conv1_x)\n",
        "      if self.use_dropout:\n",
        "          conv1_x = self.dropout(conv1_x)\n",
        "      conv1_x = self.activation(conv1_x)\n",
        "      conv2_x = self.conv_2(conv1_x)\n",
        "      if self.use_bn:\n",
        "          conv2_x = self.batchnorm(conv2_x)\n",
        "      if self.use_dropout:\n",
        "          conv2_x = self.dropout(conv2_x)\n",
        "\n",
        "      return self.activation(conv2_x + residual)\n",
        "\n",
        "class ClothingEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(ClothingEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "\n",
        "      return x0, x1, x2, x3, x4\n",
        "\n",
        "class SegmentEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "      super(SegmentEncoder, self).__init__()\n",
        "      self.resnet1 = ResNetEncoderBlock(input_channels, output_channels)\n",
        "      self.resnet2 = ResNetEncoderBlock(output_channels, output_channels * 2)\n",
        "      self.resnet3 = ResNetEncoderBlock(output_channels * 2, output_channels * 4)\n",
        "      self.resnet4 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet5 = ResNetEncoderBlock(output_channels * 4, output_channels * 4)\n",
        "      self.resnet6 = ResNetEncoderBlock(output_channels * 4, output_channels * 8, down=False)\n",
        "      self.resnet7 = ResNetEncoderBlock(output_channels * 8, output_channels * 4, down=False, up=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      x0 = self.resnet1(x)\n",
        "      x1 = self.resnet2(x0)\n",
        "      x2 = self.resnet3(x1)\n",
        "      x3 = self.resnet4(x2)\n",
        "      x4 = self.resnet5(x3)\n",
        "      x5 = self.resnet6(x4)\n",
        "      x6 = self.resnet7(x5)\n",
        "\n",
        "      return x0, x1, x2, x3, x4, x5, x6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WarpingProcess(nn.Module):\n",
        "\n",
        "  def __init__(self, oc):\n",
        "      super(WarpingProcess, self).__init__()\n",
        "\n",
        "      self.conv_after_concat_1 = nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "\n",
        "      self.convs_clothing = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc*2, oc*4, kernel_size=1),\n",
        "          nn.Conv2d(oc, oc*4, kernel_size=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment = nn.ModuleList([\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*4, oc*4, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.conv_after_concat_2 = nn.ModuleList([\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1),\n",
        "          nn.Conv2d(oc*8, 2, kernel_size=3, padding=1)\n",
        "      ])\n",
        "\n",
        "      self.convs_segment_after_concat = nn.ModuleList([\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*12, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*10, oc*4, down=False, up=True),\n",
        "          ResNetEncoderBlock(oc*9, oc*4, down=False, up=True)\n",
        "      ])\n",
        "\n",
        "      self.clothingencoder = ClothingEncoder(4, oc)\n",
        "      self.posencoder = SegmentEncoder(6, oc)\n",
        "\n",
        "  def forward(self, input_1, input_2, device):\n",
        "\n",
        "    clothing_before_warp = []\n",
        "    pose_after_concat = []\n",
        "    clothingpose_before_warp = []\n",
        "\n",
        "    flow_list = []\n",
        "\n",
        "    ce_0, ce_1, ce_2, ce_3, ce_4 = self.clothingencoder(input_1)\n",
        "    clothing_features = [ce_0, ce_1, ce_2, ce_3, ce_4]\n",
        "\n",
        "    pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6 = self.posencoder(input_2)\n",
        "    pose_features = [pe_0, pe_1, pe_2, pe_3, pe_4, pe_5, pe_6]\n",
        "\n",
        "    clothing_pose_last_feature = torch.cat([ce_4, pe_4], dim=1)\n",
        "\n",
        "    conv_before_warp = self.conv_after_concat_1(clothing_pose_last_feature)\n",
        "\n",
        "    for i in range(4):\n",
        "\n",
        "      if i == 0:\n",
        "\n",
        "        up = F.interpolate(clothing_features[4], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(conv_before_warp, scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_features[6], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_features[6])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "      else:\n",
        "\n",
        "        up = F.interpolate(clothing_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        conv_up = self.convs_clothing[i](clothing_features[3 - i]) + up\n",
        "\n",
        "        clothing_before_warp.append(conv_up)\n",
        "\n",
        "        up_flow = F.interpolate(clothingpose_before_warp[i - 1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "        grid = make_grid(1, up_flow.shape[2], up_flow.shape[3], device)\n",
        "\n",
        "        flow_norm = torch.cat([up_flow[:, 0:1, :, :] / ((up_flow.shape[3] - 1.0) / 2.0), up_flow[:, 1:2, :, :] / ((up_flow.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "        warped_T1 = F.grid_sample(conv_up, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "        flow_list.append(flow_norm)\n",
        "\n",
        "        pe_concat = torch.cat([pose_after_concat[i - 1], warped_T1, pose_features[3 - i]], dim=1)\n",
        "\n",
        "        conv_pe_6_out = self.convs_segment[i](pose_after_concat[i - 1])\n",
        "\n",
        "        con_pe_6_warp = torch.concat([conv_pe_6_out, warped_T1], dim=1)\n",
        "\n",
        "        conv_pe6_warp_out = self.conv_after_concat_2[i](con_pe_6_warp)\n",
        "\n",
        "        concat_up_conv_pe6_warp = up_flow + conv_pe6_warp_out\n",
        "\n",
        "        clothingpose_before_warp.append(concat_up_conv_pe6_warp)\n",
        "\n",
        "        pe_last_resblock = self.convs_segment_after_concat[i](pe_concat)\n",
        "\n",
        "        pose_after_concat.append(pe_last_resblock)\n",
        "\n",
        "    last_up = F.interpolate(clothingpose_before_warp[-1], scale_factor=2, mode='bilinear')\n",
        "\n",
        "    grid = make_grid(1, last_up.shape[2], last_up.shape[3], device)\n",
        "\n",
        "    flow_norm = torch.cat([last_up[:, 0:1, :, :] / ((last_up.shape[3] - 1.0) / 2.0), last_up[:, 1:2, :, :] / ((last_up.shape[2] - 1.0) / 2.0)], 1).permute(0, 2, 3, 1)\n",
        "    warped_T1 = F.grid_sample(input_1, grid + flow_norm, padding_mode='border')\n",
        "\n",
        "    flow_list.append(flow_norm)\n",
        "\n",
        "    warped_c = warped_T1[:, :-1, :, :]\n",
        "    warped_cm = warped_T1[:, -1:, :, :]\n",
        "\n",
        "    return warped_c, warped_cm, flow_list\n"
      ],
      "metadata": {
        "id": "BaVhm1GlBbiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCgvCZwc_mLw"
      },
      "outputs": [],
      "source": [
        "class WarpingCloth():\n",
        "\n",
        "  def __init__(self, size, checkpoint_warping):\n",
        "\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    self.checkpoint_warping = checkpoint_warping\n",
        "\n",
        "    self.size = size\n",
        "\n",
        "    self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.size),        # Resize to fixed size\n",
        "            transforms.ToTensor(),          # Converts to [C, H, W], values in [0, 1]\n",
        "        ])\n",
        "\n",
        "    self.generator = WarpingProcess(96).to(self.device)\n",
        "    self.gen_opt = torch.optim.Adam(self.generator.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
        "\n",
        "  def warped_cloth_into_agnostic(self, agnostic_image, warped_cloth_image, warped_cloth_mask):\n",
        "\n",
        "    agnostic_image_warped_cloth = agnostic_image.clone()\n",
        "\n",
        "    warped_cloth_mask[warped_cloth_mask > 0.5] = 1\n",
        "\n",
        "    warped_cloth_mask = warped_cloth_mask.repeat(1, 3, 1, 1)\n",
        "\n",
        "    agnostic_image_warped_cloth[warped_cloth_mask == 1] = warped_cloth_image[warped_cloth_mask == 1]\n",
        "\n",
        "    return agnostic_image_warped_cloth\n",
        "\n",
        "  def warping(self, checkpoint_warping):\n",
        "\n",
        "    self.generator.load_state_dict(self.checkpoint_warping['model_state_dict'])\n",
        "    self.gen_opt.load_state_dict(self.checkpoint_warping['optimizer_state_dict'])\n",
        "\n",
        "    warped_c, warped_cm, flow_list = self.generator(self.input_1, self.input_2, self.device)\n",
        "\n",
        "    new_im = self.warped_cloth_into_agnostic(self.agnostic, warped_c, warped_cm)\n",
        "\n",
        "    im_reshaped = F.interpolate(new_im, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "    return im_reshaped\n",
        "\n",
        "  def __call__(self, person_img_path, cloth_img_path):\n",
        "\n",
        "    img_preprocess = Preprocessing(person_img_path, cloth_img_path)\n",
        "\n",
        "    open_pose = img_preprocess.open_pose()[:, :, ::-1]\n",
        "\n",
        "    key_points = img_preprocess.key_points()\n",
        "\n",
        "    parse_human = img_preprocess.parse_human()\n",
        "\n",
        "    agnostic, agnostic_mask = img_preprocess.get_agnostic_and_mask()\n",
        "\n",
        "    parse_agnostic = img_preprocess.get_parse_agnostic()\n",
        "    parse_agnostic[parse_agnostic != parse_human] = 0\n",
        "\n",
        "    cloth_mask = img_preprocess.cloth_mask()\n",
        "\n",
        "    self.cloth = self.transform(img_preprocess.cloth_img).unsqueeze(0).to(self.device)\n",
        "    self.cloth_mask = self.transform(Image.fromarray(cloth_mask)).unsqueeze(0).to(self.device)\n",
        "    self.dense_pose = self.transform(Image.fromarray(open_pose)).unsqueeze(0).to(self.device)\n",
        "    self.parse_agnostic = self.transform(Image.fromarray(parse_agnostic)).unsqueeze(0).to(self.device)\n",
        "    self.agnostic = self.transform(Image.fromarray(agnostic)).unsqueeze(0).to(self.device)\n",
        "    # This image is just converted from array to image to be the input of DDIM\n",
        "    self.agnostic_mask = Image.fromarray(agnostic_mask)\n",
        "\n",
        "    self.input_1 = torch.cat([self.cloth, self.cloth_mask[:, 0:1, ...]], dim=1)\n",
        "    # The parse agnostic image is multiplied by 15 for have the correct normalization.\n",
        "    self.input_2 = torch.cat([self.dense_pose, (self.parse_agnostic)*15], dim=1)\n",
        "\n",
        "    output = self.warping(self.checkpoint_warping)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  person_img_path = '00509_00.jpg'\n",
        "  cloth_img_path = '00126_00.jpg'\n",
        "\n",
        "  check_point_path ='/content/drive/MyDrive/AIClothes/DDPM/Models/Warping/3000imgs_warping_lr5e-05_vgg0.1_tvlmabda_1epoch_30.pth'\n",
        "  checkpoint_warping = torch.load(check_point_path, map_location=torch.device(device))\n",
        "\n",
        "  warping = WarpingCloth((256, 256), checkpoint_warping)\n",
        "\n",
        "  output = warping(person_img_path, cloth_img_path)\n",
        "\n",
        "  %matplotlib inline\n",
        "\n",
        "  plt.imshow(output[0].permute(1, 2, 0).detach().cpu().numpy())\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rXrlD-3vyy6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3hLSqdW_00g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}