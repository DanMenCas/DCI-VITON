{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1245,"status":"ok","timestamp":1745340571486,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"},"user_tz":300},"id":"JfkorNJrnmNO","outputId":"bcf040ef-d446-4f84-d078-09e354bab3c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7b0954451c50>"]},"metadata":{},"execution_count":8}],"source":["import torch\n","import numpy as np\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","from torchvision.models import vgg19\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","from google.colab.patches import cv2_imshow\n","from google.colab import drive\n","drive.mount('/content/drive')\n","torch.manual_seed(0)"]},{"cell_type":"code","source":["def load_image(image_path, size=(576, 576)):\n","    \"\"\"\n","    Loads a single image and converts it to a tensor of shape [1, 3, H, W]\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize(size),        # Resize to fixed size\n","        transforms.ToTensor(),          # Converts to [C, H, W], values in [0, 1]\n","        # transforms.Normalize([0.5]*3, [0.5]*3)  # Optional if using Tanh output\n","    ])\n","\n","    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n","    image = transform(image)                      # [3, H, W]\n","\n","    return image\n","\n","def tensor_create(image):\n","\n","    tensor = image.unsqueeze(0)                   # Add batch dimension → [1, 3, H, W]\n","\n","    return tensor"],"metadata":{"id":"TyZ9hUxWWR0Q","executionInfo":{"status":"ok","timestamp":1745340572186,"user_tz":300,"elapsed":6,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"fe1-kOhTpRL-","executionInfo":{"status":"ok","timestamp":1745340617395,"user_tz":300,"elapsed":43646,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["images_edited = []\n","\n","images_real = []\n","\n","images_edited_mask = []\n","\n","images_real_mask = []\n","\n","dup = 20\n","\n","for _ in range(dup):\n","    for i in range(13):\n","\n","        real_image = load_image(\"/content/drive/MyDrive/AIClothes/Pants/OriginalImages/PantsOn/PantsOn_\" + (\"00\" if len(str(i+1)) == 1 else \"0\") + str(i+1) + \".jpeg\")\n","\n","        edited_image = load_image(\"/content/drive/MyDrive/AIClothes/Pants/FinalInput/EditedPantsOn_\" + (\"00\" if len(str(i+1)) == 1 else \"0\") + str(i+1) + \".jpg\")\n","\n","        mask_image = load_image(\"/content/drive/MyDrive/AIClothes/Pants/Annotations/PantsOn/PantsOn_\" + (\"00\" if len(str(i+1)) == 1 else \"0\") + str(i+1) + \"_gtFine_labelIds.png\")\n","\n","        #Resize the image to be comparable\n","\n","        mask_tensor = tensor_create(mask_image*255)\n","\n","        image_tensor_edited = tensor_create(edited_image)\n","\n","        image_tensor_edited_mask = tensor_create(edited_image*(mask_image*255))\n","\n","        image_tensor_real = tensor_create(real_image)\n","\n","        image_tensor_real_mask = tensor_create(real_image * (mask_image*255))\n","\n","        images_edited.append(image_tensor_edited)\n","\n","        images_edited_mask.append(image_tensor_edited_mask)\n","\n","        images_real.append(image_tensor_real)\n","\n","        images_real_mask.append(image_tensor_real_mask)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xvY4ZNyUviY9","executionInfo":{"status":"ok","timestamp":1745340622580,"user_tz":300,"elapsed":114,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["class ContractingBlock(nn.Module):\n","  '''\n","  ContractingBlock Class\n","  Performs two convolutions followed by a max pool operation.\n","  Values:\n","      input_channels: the number of channels to expect from a given input\n","  '''\n","  def __init__(self, input_channels, use_dropout=False, use_bn=True):\n","      super(ContractingBlock, self).__init__()\n","      self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=3, stride=2, padding=1)\n","      self.activation = nn.LeakyReLU(0.2)\n","      if use_bn:\n","          self.batchnorm = nn.InstanceNorm2d(input_channels * 2)\n","      self.use_bn = use_bn\n","      if use_dropout:\n","          self.dropout = nn.Dropout()\n","      self.use_dropout = use_dropout\n","\n","  def forward(self, x):\n","      '''\n","      Function for completing a forward pass of ContractingBlock:\n","      Given an image tensor, completes a contracting block and returns the transformed tensor.\n","      Parameters:\n","          x: image tensor of shape (batch size, channels, height, width)\n","      '''\n","      x = self.conv1(x)\n","      if self.use_bn:\n","          x = self.batchnorm(x)\n","      if self.use_dropout:\n","          x = self.dropout(x)\n","      x = self.activation(x)\n","      return x\n","\n","class ExpandingBlock(nn.Module):\n","  '''\n","  ExpandingBlock Class:\n","  Performs an upsampling, a convolution, a concatenation of its two inputs,\n","  followed by two more convolutions with optional dropout\n","  Values:\n","      input_channels: the number of channels to expect from a given input\n","  '''\n","  def __init__(self, input_channels, use_dropout=False, use_bn=True):\n","      super(ExpandingBlock, self).__init__()\n","      #self.upsample = nn.ConvTranspose2d(input_channels, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n","      self.upsample = nn.ConvTranspose2d(input_channels, input_channels, kernel_size=2, stride=2)\n","      self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)\n","      if use_bn:\n","          self.batchnorm = nn.InstanceNorm2d(input_channels // 2, affine=True)\n","      self.use_bn = use_bn\n","      self.activation = nn.ReLU()\n","      if use_dropout:\n","          self.dropout = nn.Dropout()\n","      self.use_dropout = use_dropout\n","\n","  def forward(self, x, skip_con_x):\n","      '''\n","      Function for completing a forward pass of ExpandingBlock:\n","      Given an image tensor, completes an expanding block and returns the transformed tensor.\n","      Parameters:\n","          x: image tensor of shape (batch size, channels, height, width)\n","          skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n","                  for the skip connection\n","      '''\n","      x = self.upsample(x)\n","      x = self.conv1(x)\n","      if self.use_bn:\n","          x = self.batchnorm(x)\n","      if self.use_dropout:\n","          x = self.dropout(x)\n","      x = torch.cat([x, skip_con_x], axis=1)\n","      x = self.conv1(x)\n","      if self.use_bn:\n","          x = self.batchnorm(x)\n","      if self.use_dropout:\n","          x = self.dropout(x)\n","      x = self.activation(x)\n","      return x\n","\n","class FeatureMapBlock(nn.Module):\n","  '''\n","  FeatureMapBlock Class\n","  The final layer of a U-Net -\n","  maps each pixel to a pixel with the correct number of output dimensions\n","  using a 1x1 convolution.\n","  Values:\n","      input_channels: the number of channels to expect from a given input\n","      output_channels: the number of channels to expect for a given output\n","  '''\n","  def __init__(self, input_channels, output_channels):\n","      super(FeatureMapBlock, self).__init__()\n","      self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n","\n","  def forward(self, x):\n","      '''\n","      Function for completing a forward pass of FeatureMapBlock:\n","      Given an image tensor, returns it mapped to the desired number of channels.\n","      Parameters:\n","          x: image tensor of shape (batch size, channels, height, width)\n","      '''\n","      x = self.conv(x)\n","      return x\n","\n","class UNet(nn.Module):\n","  '''\n","  UNet Class\n","  A series of 4 contracting blocks followed by 4 expanding blocks to\n","  transform an input image into the corresponding paired image, with an upfeature\n","  layer at the start and a downfeature layer at the end.\n","  Values:\n","      input_channels: the number of channels to expect from a given input\n","      output_channels: the number of channels to expect for a given output\n","  '''\n","  def __init__(self, input_channels, output_channels, hidden_channels=32):\n","      super(UNet, self).__init__()\n","      self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","      self.contract1 = ContractingBlock(hidden_channels) # this layer could have dropout\n","      self.contract2 = ContractingBlock(hidden_channels * 2) # this layer could have dropout\n","      self.contract3 = ContractingBlock(hidden_channels * 4) # this layer could have dropout\n","      self.contract4 = ContractingBlock(hidden_channels * 8)\n","      self.contract5 = ContractingBlock(hidden_channels * 16)\n","      self.contract6 = ContractingBlock(hidden_channels * 32)\n","      self.expand0 = ExpandingBlock(hidden_channels * 64)\n","      self.expand1 = ExpandingBlock(hidden_channels * 32)\n","      self.expand2 = ExpandingBlock(hidden_channels * 16)\n","      self.expand3 = ExpandingBlock(hidden_channels * 8)\n","      self.expand4 = ExpandingBlock(hidden_channels * 4)\n","      self.expand5 = ExpandingBlock(hidden_channels * 2)\n","      self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n","      self.sigmoid = torch.nn.Sigmoid()\n","\n","  def forward(self, x):\n","      '''\n","      Function for completing a forward pass of UNet:\n","      Given an image tensor, passes it through U-Net and returns the output.\n","      Parameters:\n","          x: image tensor of shape (batch size, channels, height, width)\n","      '''\n","      x0 = self.upfeature(x)\n","      x1 = self.contract1(x0)\n","      x2 = self.contract2(x1)\n","      x3 = self.contract3(x2)\n","      x4 = self.contract4(x3)\n","      x5 = self.contract5(x4)\n","      x6 = self.contract6(x5)\n","      x7 = self.expand0(x6, x5)\n","      x8 = self.expand1(x7, x4)\n","      x9 = self.expand2(x8, x3)\n","      x10 = self.expand3(x9, x2)\n","      x11 = self.expand4(x10, x1)\n","      x12 = self.expand5(x11, x0)\n","      xn = self.downfeature(x12)\n","      return self.sigmoid(xn)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Pp-vTu-9-ksz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745340628965,"user_tz":300,"elapsed":5167,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}},"outputId":"3fd11e54-e27a-445b-9baf-e7f50c958e2f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 576, 576])"]},"metadata":{},"execution_count":12}],"source":["gen = UNet(3, 3).to('cpu')\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=0.0002)\n","img = torch.randn(1, 3, 576, 576)\n","out = gen(img)\n","out.shape"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0nVuJPjV1f92","executionInfo":{"status":"ok","timestamp":1745340628972,"user_tz":300,"elapsed":5,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","\n","    def __init__(self, input_channels, hidden_channels=32):\n","        super(Discriminator, self).__init__()\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels, use_bn=True)\n","        self.contract2 = ContractingBlock(hidden_channels * 2)\n","        self.contract3 = ContractingBlock(hidden_channels * 4)\n","        self.contract4 = ContractingBlock(hidden_channels * 8)\n","        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=(1,1))\n","\n","    def forward(self, x, y):\n","        x = torch.cat([x, y], axis=1)\n","        x0 = self.upfeature(x)\n","        x1 = self.contract1(x0)\n","        x2 = self.contract2(x1)\n","        x3 = self.contract3(x2)\n","        x4 = self.contract4(x3)\n","        xn = self.final(x4)\n","        return xn"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UXptQZcwrBrq","executionInfo":{"status":"ok","timestamp":1745340628993,"user_tz":300,"elapsed":18,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["import torch.nn.functional as F\n","# New parameters\n","adv_criterion = nn.BCEWithLogitsLoss()\n","recon_criterion_l1 = nn.L1Loss()\n","recon_criterion_l2 = nn.MSELoss()\n","l1_recon = 100\n","l2_recon = 10\n","lambda_perc = 1\n","\n","n_epochs = 30\n","input_dim = 3\n","real_dim = 3\n","display_step = 200\n","batch_size = 4\n","lr = 0.0002\n","target_shape = 448\n","device = 'cpu'"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"PNAK2XqMJ419","executionInfo":{"status":"ok","timestamp":1745340632094,"user_tz":300,"elapsed":11,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["# transform = transforms.Compose([\n","#     transforms.ToTensor(),\n","# ])\n","\n","# import torchvision\n","# dataset = torchvision.datasets.ImageFolder(\"maps\", transform=transform)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"vBY3Y9UrUgVX","executionInfo":{"status":"ok","timestamp":1745340634194,"user_tz":300,"elapsed":990,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["gen = UNet(input_dim, real_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n","disc = Discriminator(input_dim + real_dim).to(device)\n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.xavier_normal_(m.weight)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.xavier_normal_(m.weight)\n","        torch.nn.init.constant_(m.bias, 0)\n","\n","# Feel free to change pretrained to False if you're training the model from scratch\n","pretrained = False\n","if pretrained:\n","    loaded_state = torch.load(\"/content/drive/MyDrive/AIClothes/PreTrainedModels/pix2pix_15000.pth\")\n","    gen.load_state_dict(loaded_state[\"gen\"])\n","    gen_opt.load_state_dict(loaded_state[\"gen_opt\"])\n","    disc.load_state_dict(loaded_state[\"disc\"])\n","    disc_opt.load_state_dict(loaded_state[\"disc_opt\"])\n","else:\n","    gen = gen.apply(weights_init)\n","    disc = disc.apply(weights_init)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"NwORrB6mnxms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745340646942,"user_tz":300,"elapsed":10718,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}},"outputId":"ca1284b0-4f92-4213-dcc9-120a3c74de2a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:08<00:00, 67.4MB/s]\n"]}],"source":["# VGG19 feature extractor for specific layers\n","class VGG19Features(nn.Module):\n","    def __init__(self, layer_ids=[12, 21, 30]):\n","        super().__init__()\n","        self.vgg = vgg19(pretrained=True).features\n","        self.layer_ids = layer_ids\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False  # Freeze weights\n","\n","    def forward(self, x):\n","        features = []\n","        for i, layer in enumerate(self.vgg):\n","            x = layer(x)\n","            if i in self.layer_ids:\n","                features.append(x)\n","        return features\n","\n","# Preprocessing for VGG19\n","preprocess = transforms.Compose([\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","def prepare_image(img):\n","    # Assuming img is [0, 1], shape [batch, 3, H, W]\n","    img = img.clamp(0, 1)  # Ensure valid range\n","    img = preprocess(img)  # Normalize for VGG\n","    return img\n","\n","# Perceptual loss function\n","def perceptual_loss(generated, target, vgg_extractor, weights=[1.0, 1.0, 1.0]):\n","    gen_features = vgg_extractor(prepare_image(generated))\n","    target_features = vgg_extractor(prepare_image(target))\n","\n","    loss = 0\n","    for gen_f, tar_f, w in zip(gen_features, target_features, weights):\n","        loss += w * torch.mean(torch.abs(gen_f - tar_f))  # L1 loss per layer\n","    return loss / len(weights)\n","\n","# Example usage\n","vgg_extractor = VGG19Features(layer_ids=[12, 21, 30]).eval()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"YZE-Eyj0LOpm","executionInfo":{"status":"ok","timestamp":1745340648825,"user_tz":300,"elapsed":2,"user":{"displayName":"Daniel Mendoza Castrillón","userId":"02504609499046023435"}}},"outputs":[],"source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: get_gen_loss\n","def get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion_l1, recon_criterion_l2, l1_recon, l2_recon, lambda_perc, vgg_extractor):\n","    '''\n","    Return the loss of the generator given inputs.\n","    Parameters:\n","        gen: the generator; takes the condition and returns potential images\n","        disc: the discriminator; takes images and the condition and\n","          returns real/fake prediction matrices\n","        real: the real images (e.g. maps) to be used to evaluate the reconstruction\n","        condition: the source images (e.g. satellite imagery) which are used to produce the real images\n","        adv_criterion: the adversarial loss function; takes the discriminator\n","                  predictions and the true labels and returns a adversarial\n","                  loss (which you aim to minimize)\n","        recon_criterion: the reconstruction loss function; takes the generator\n","                    outputs and the real images and returns a reconstructuion\n","                    loss (which you aim to minimize)\n","        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum\n","    '''\n","    # Steps: 1) Generate the fake images, based on the conditions.\n","    gen_img = gen(condition)\n","    #        2) Evaluate the fake images and the condition with the discriminator.\n","    disc_fake_pred = disc(gen_img, condition)\n","    #        3) Calculate the adversarial and reconstruction losses.\n","    adv = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n","    recon_l1 = recon_criterion_l1(real, gen_img)\n","    recon_l2 = recon_criterion_l2(real, gen_img)\n","    #        4) Add the two losses, weighting the reconstruction loss appropriately.\n","\n","    #### START CODE HERE ####\n","    if lambda_perc != 0:\n","\n","        gen_loss = adv + (recon_l1*l1_recon) + (recon_l2*l2_recon) + (perceptual_loss(gen_img, real, vgg_extractor)*lambda_perc)\n","\n","    else:\n","\n","        gen_loss = adv + (recon_l1*l1_recon) + (recon_l2*l2_recon)\n","\n","    #### END CODE HERE ####\n","    return gen_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy6UBV60HtnY"},"outputs":[],"source":["#from skimage import color\n","import numpy as np\n","\n","\n","mean_generator_loss = 0\n","mean_discriminator_loss = 0\n","#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","cur_step = 0\n","\n","for epoch in range(n_epochs):\n","    # Dataloader returns the batches\n","    for i in range(len(images_edited)):\n","        condition = images_edited_mask[i]/1.\n","\n","        real = images_real_mask[i]/1.\n","\n","        cur_batch_size = len(condition)\n","        condition = condition.to(device)\n","        real = real.to(device)\n","\n","        ### Update discriminator ###\n","        disc_opt.zero_grad() # Zero out the gradient before backpropagation\n","        with torch.no_grad():\n","            fake = gen(condition)\n","        disc_fake_hat = disc(fake.detach(), condition) # Detach generator\n","        disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n","        disc_real_hat = disc(real, condition)\n","        disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n","        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","        disc_loss.backward(retain_graph=True) # Update gradients\n","        disc_opt.step() # Update optimizer\n","\n","        ### Update generator ###\n","        gen_opt.zero_grad()\n","        gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion_l1, recon_criterion_l2, l1_recon, l2_recon, lambda_perc, vgg_extractor)\n","        gen_loss.backward() # Update gradients\n","        gen_opt.step() # Update optimizer\n","\n","        # Keep track of the average discriminator loss\n","        mean_discriminator_loss += disc_loss.item() / display_step\n","        # Keep track of the average generator loss\n","        mean_generator_loss += gen_loss.item() / display_step\n","\n","        ### Visualization code ###\n","        # if cur_step % display_step == 0:\n","        #     if cur_step > 0:\n","        #         print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n","        #     else:\n","        #         print(\"Pretrained initial state\")\n","        #     show_tensor_images(condition, size=(input_dim, target_shape, target_shape))\n","        #     show_tensor_images(real, size=(real_dim, target_shape, target_shape))\n","        #     show_tensor_images(fake, size=(real_dim, target_shape, target_shape))\n","        #     mean_generator_loss = 0\n","        #     mean_discriminator_loss = 0\n","        #     # You can change save_model to True if you'd like to save the model\n","        #     if save_model:\n","        #         torch.save({'gen': gen.state_dict(),\n","        #             'gen_opt': gen_opt.state_dict(),\n","        #             'disc': disc.state_dict(),\n","        #             'disc_opt': disc_opt.state_dict()\n","        #         }, f\"pix2pix_{cur_step}.pth\")\n","        cur_step += 1\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns\n","\n","    # Display each image in a subplot\n","    axes[0].imshow(fake[0].detach().cpu().permute(1, 2, 0))\n","    axes[0].set_title(\"GAN Output\")\n","    axes[0].axis('off')  # Hide axes\n","\n","    axes[1].imshow(condition[0].detach().cpu().permute(1, 2, 0))\n","    axes[1].set_title(\"Input Image\")\n","    axes[1].axis('off')\n","\n","    axes[2].imshow(real[0].detach().cpu().permute(1, 2, 0))\n","    axes[2].set_title(\"Real Image\")\n","    axes[2].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmyA_RgM6spB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1QKfOWmSGoqFgsvdKMXhcFZRYlDE2wKQ4","timestamp":1729173104899}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}