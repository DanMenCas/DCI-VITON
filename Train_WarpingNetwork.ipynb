{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxuQhCA2J3vc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from warpingnetwork import WarpingProcess\n",
    "from losses import VGGLoss, flow_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_Voy5RwYvME"
   },
   "outputs": [],
   "source": [
    "def make_grid(N, iW, iH, device):\n",
    "    grid_x = torch.linspace(-1.0, 1.0, iW).view(1, 1, iW, 1).expand(N, iH, -1, -1).to(device)\n",
    "    grid_y = torch.linspace(-1.0, 1.0, iH).view(1, iH, 1, 1).expand(N, -1, iW, -1).to(device)\n",
    "    grid = torch.cat([grid_x, grid_y], 3)\n",
    "    return grid\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def warped_cloth_into_agnostic(agnostic_image, warped_cloth_image, warped_cloth_mask):\n",
    "\n",
    "  agnostic_image_warped_cloth = agnostic_image.clone()\n",
    "\n",
    "  warped_cloth_mask[warped_cloth_mask > 0.5] = 1\n",
    "\n",
    "  warped_cloth_mask = warped_cloth_mask.repeat(1, 3, 1, 1)\n",
    "\n",
    "  agnostic_image_warped_cloth[warped_cloth_mask == 1] = warped_cloth_image[warped_cloth_mask == 1]\n",
    "\n",
    "  return agnostic_image_warped_cloth"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "g6Zk4gihkqaL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "warped_masks = torch.load('data/warping/warped_masks.pth').to(device)\n",
    "agnostics = torch.load('data/warping/agnostics.pth').to(device)\n",
    "agnostic_masks = torch.load('data/warping/agnostic_masks.pth').to(device)\n",
    "inputs_1 = torch.load('data/warping/inputs_1.pth').to(device)\n",
    "inputs_2 = torch.load('data/warping/inputs_2.pth').to(device)"
   ],
   "metadata": {
    "id": "5AZAIhwJ3DL6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "warped_masks = torch.unbind(warped_masks, dim=0)\n",
    "agnostics = torch.unbind(agnostics, dim=0)\n",
    "agnostic_masks = torch.unbind(agnostic_masks, dim=0)\n",
    "inputs_1 = torch.unbind(inputs_1, dim=0)\n",
    "inputs_2 = torch.unbind(inputs_2, dim=0)"
   ],
   "metadata": {
    "id": "0UCEVduy3Jh-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "warped_cloths = [((i + 1 )/ 2)*j for i, j in zip(warped_masks, images)]"
   ],
   "metadata": {
    "id": "9n1_EKzSbNgh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "agnostics = [i.unsqueeze(0) for i in agnostics]\n",
    "agnostic_masks = [i.unsqueeze(0) for i in agnostic_masks]\n",
    "inputs_1 = [i.unsqueeze(0) for i in inputs_1]\n",
    "inputs_2 = [i.unsqueeze(0) for i in inputs_2]"
   ],
   "metadata": {
    "id": "SY5YOwEF3K6h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNlDo--DaNFd"
   },
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "l1_lambda = 1\n",
    "vgg_lambda = 0.1\n",
    "tvlambda = 1\n",
    "epochs = 30\n",
    "criterionL1 = nn.L1Loss().to(device)\n",
    "criterionVGG = VGGLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1Va22n5ajiv"
   },
   "outputs": [],
   "source": [
    "generator = WarpingProcess(96).to(device)\n",
    "gen_opt = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "generator = generator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN76XwRJlsvt"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs+1):\n",
    "\n",
    "  for input_1, input_2, warped_cloth, warped_mask in zip(inputs_1, inputs_2, warped_cloths, warped_masks):\n",
    "\n",
    "    warped_c, warped_cm, flow_list = generator(input_1, input_2, device)\n",
    "\n",
    "    loss_l1_cloth = criterionL1(warped_cm, warped_mask)\n",
    "\n",
    "    loss_vgg = criterionVGG(warped_cm*warped_c, warped_cloth)\n",
    "\n",
    "    loss_tv = flow_loss(flow_list)\n",
    "\n",
    "    # multi-scale flow prediction\n",
    "\n",
    "    for j in range(len(flow_list)-1):\n",
    "      flow = flow_list[j]\n",
    "      N, fH, fW, _ = flow.size()\n",
    "      grid = make_grid(N, input_1.shape[2], input_1.shape[3], device)\n",
    "      flow = F.interpolate(flow.permute(0, 3, 1, 2), size = input_1.shape[2:], mode='bilinear').permute(0, 2, 3, 1)\n",
    "      flow_norm = torch.cat([flow[:, :, :, 0:1] / ((flow.shape[1] - 1.0) / 2.0), flow[:, :, :, 1:2] / ((flow.shape[2] - 1.0) / 2.0)], 3)\n",
    "      warped = F.grid_sample(input_1, grid + flow_norm, padding_mode='border')\n",
    "      warped_c_flow = warped[:, :-1, :, :]\n",
    "      warped_cm_flow = warped[:, -1:, :, :]\n",
    "\n",
    "      loss_l1_cloth += criterionL1(warped_cm_flow, warped_mask) / (2 ** (4-j))\n",
    "      loss_vgg += criterionVGG(warped_cm_flow*warped_c_flow, warped_cloth) / (2 ** (4-j))\n",
    "\n",
    "\n",
    "    loss_G = ((l1_lambda * loss_l1_cloth) + (vgg_lambda * loss_vgg) + (tvlambda * loss_tv))\n",
    "\n",
    "    losses.append(loss_G.item())\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    loss_G.backward()\n",
    "    gen_opt.step()\n",
    "\n",
    "    # Visualization of the results\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "\n",
    "    #   fig, axs = plt.subplots(1, 7, figsize=(10, 5))\n",
    "    #   axs[0].imshow(warped_c[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[0].axis('off')\n",
    "\n",
    "    #   axs[1].imshow(warped_cloth[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[1].axis('off')\n",
    "\n",
    "    #   axs[2].imshow(warped_cm[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[2].axis('off')\n",
    "\n",
    "    #   axs[3].imshow(warped_mask[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[3].axis('off')\n",
    "\n",
    "    #   axs[4].imshow(input_1[0, 0:3, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[4].axis('off')\n",
    "\n",
    "    #   axs[5].imshow(input_2[0, 0:3, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[5].axis('off')\n",
    "\n",
    "    #   axs[6].imshow(input_2[0, 3:6, ...].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #   axs[6].axis('off')\n",
    "\n",
    "    #   plt.tight_layout()\n",
    "    #   plt.show()\n",
    "\n",
    "  print(f'Epoch: {i}, Mean Loss: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Add the warped cloth output of the model into the agnostic image.\n",
    "\n",
    "inputs_diffusion_model = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for img in range(len(agnostics)):\n",
    "\n",
    "    agnostic_image = agnostics[img]\n",
    "\n",
    "    warped_c, warped_cm, flow_list = generator(inputs_1[img], inputs_2[img], device)\n",
    "\n",
    "    new_im = warped_cloth_into_agnostic(agnostic_image, warped_c, warped_cm)\n",
    "\n",
    "    new_im = F.interpolate(new_im, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "    inputs_diffusion_model.append(new_im)"
   ],
   "metadata": {
    "id": "y7x1RugJgJki"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample of 2k images for the diffusion model\n",
    "\n",
    "list_sample_inputs_diffusion_model = random.sample([i for i in range(3000)], 2000)\n",
    "\n",
    "sample_inputs_diffusion_model = []\n",
    "\n",
    "for i in list_sample_inputs_diffusion_model:\n",
    "  sample_inputs_diffusion_model.append(inputs_diffusion_model[i])"
   ],
   "metadata": {
    "id": "zPhegPW9p5xZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "torch.save(torch.cat(inputs_diffusion_model, dim=0), 'data/input_difussion_model_sample.pth')",
   "metadata": {
    "id": "2vV1crApZKi6"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
